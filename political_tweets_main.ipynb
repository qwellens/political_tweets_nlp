{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6864FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KWb0m_Eh_p5l"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez0bGAAtTO9z",
        "colab_type": "code",
        "outputId": "a2681fd0-720c-41d5-8139-47c761880229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF68fgflrD4u",
        "colab_type": "text"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n",
        "\n",
        "https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/train.py\n",
        "\n",
        "https://arxiv.org/abs/1408.5882\n",
        "\n",
        "https://www.kaggle.com/leighplt/pytorch-torchtext-glove\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#conv2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx0ANdomX_Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, utils\n",
        "import random\n",
        "SEED = 1200\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3j36TMJe1Sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "clintontrump_path = \"/content/drive/My Drive/6.864/finalproject/clinton_trump_clean.csv\"\n",
        "alltweets_path = \"/content/drive/My Drive/6.864/finalproject/ExtractedTweets.csv\"\n",
        "\n",
        "clintontrump = pd.read_csv(clintontrump_path)\n",
        "alltweets = pd.read_csv(alltweets_path)\n",
        "# clintontrump = clintontrump[['id', 'handle', 'text']].rename(columns={'id':'Party', 'handle':'Handle', 'text':'Tweet'})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec5luumofqYV",
        "colab_type": "code",
        "outputId": "8ddfb6b5-2936-4123-b018-8c6b25739c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(alltweets[0:1][\"Tweet\"])\n",
        "print(alltweets.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    Today, Senate Dems vote to #SaveTheInternet. P...\n",
            "Name: Tweet, dtype: object\n",
            "Index(['Party', 'Handle', 'Tweet'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZf41zaLj9di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data    \n",
        "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.long, batch_first=True)\n",
        "\n",
        "#CLTR VS ALL\n",
        "fields = [('Party', None), ('Handle',LABEL),('Tweet', TEXT)]\n",
        "# fields = [('Party', LABEL), ('Handle',None),('Tweet', TEXT)]\n",
        "\n",
        "#CLTR VS ALL\n",
        "#loading custom dataset\n",
        "# all_data=data.TabularDataset(path = clintontrump_path,format = 'csv',fields = fields,skip_header = True)\n",
        "all_data=data.TabularDataset(path = alltweets_path,format = 'csv',fields = fields,skip_header = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkldMoUhmu-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CLTR VS ALL\n",
        "train_data, valid_data, test_data = all_data.split(split_ratio=[0.7, 0.2, 0.1], stratified=True, strata_field = 'Handle', random_state = random.seed(SEED))\n",
        "# train_data, valid_data, test_data = all_data.split(split_ratio=[0.7, 0.2, 0.1], stratified=True, strata_field = 'Party', random_state = random.seed(SEED))\n",
        "\n",
        "TEXT.build_vocab(train_data, min_freq=3 , vectors = \"glove.6B.100d\")  \n",
        "LABEL.build_vocab(train_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrAm4ebbKkoC",
        "colab_type": "code",
        "outputId": "f3e432a2-743d-4a8c-a870-2487fc6a1d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqXxTHHA50Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "print(TEXT.vocab.freqs.most_common(30))  \n",
        "print(TEXT.vocab.stoi)   \n",
        "\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "NUM_CATEGORIES = len(LABEL.vocab)\n",
        "\n",
        "embs_vocab = TEXT.vocab.vectors\n",
        "\n",
        "print(NUM_CATEGORIES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Ske5R2BBwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_sizes = (BATCH_SIZE, BATCH_SIZE, len(test_data)),\n",
        "    sort_key = lambda x: len(x.Tweet),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io77cjBzFLzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, embs_vocab, embed_size, hidden_size, num_conv_layers, num_categories, dropout=0.):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    input_channels = 1\n",
        "    vocab_size = len(embs_vocab)\n",
        "    embed_size = embed_size #TODO\n",
        "    hidden_size = hidden_size\n",
        "    num_conv_layers = num_conv_layers #UNUSED\n",
        "    num_categories = num_categories \n",
        "\n",
        "    self.embed = nn.Embedding.from_pretrained(embs_vocab)\n",
        "  \n",
        "    self.conv13 = nn.Conv2d(input_channels, hidden_size, (3, embed_size))\n",
        "    self.conv14 = nn.Conv2d(input_channels, hidden_size, (4, embed_size))\n",
        "    self.conv15 = nn.Conv2d(input_channels, hidden_size, (5, embed_size))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(num_conv_layers*hidden_size, num_categories)\n",
        "\n",
        "  def conv_and_pool(self, x, conv):\n",
        "    x = F.relu(conv(x)).squeeze(3)  # (batch_size, hidden_size, sent_length)\n",
        "    x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, x_length):\n",
        "    #TODO: PADDING ???? \n",
        "    x = self.embed(x)  # (batch_size, sent_length, embed_dim) ??\n",
        "    x = x.unsqueeze(1)  # (batch_size, input_channels, sent_length, embed_dim) ??\n",
        "    # print('after unsqueeze', x.size())\n",
        "\n",
        "    x1 = self.conv_and_pool(x,self.conv13) #(batch_size, hidden_size)\n",
        "    x2 = self.conv_and_pool(x,self.conv14) #(batch_size, hidden_size)\n",
        "    x3 = self.conv_and_pool(x,self.conv15) #(batch_size, hidden_size)\n",
        "    x = torch.cat((x1, x2, x3), 1) # (batch_size, 3*hidden_size)\n",
        "\n",
        "    x = self.dropout(x)  # (batch_size, 3*hidden_size)\n",
        "    logits = self.fc(x)  # (batch_size, num_categories)\n",
        "    return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31q3VLNku8fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    iter_count = 0\n",
        "    print_every = 50\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()   \n",
        "\n",
        "        text, text_lengths = batch.Tweet   \n",
        "        \n",
        "        if text.size(1) < 5:\n",
        "          continue \n",
        "\n",
        "        predictions = model(text, text_lengths)\n",
        "        \n",
        "        #CLTR VS ALL\n",
        "        loss = criterion(predictions, batch.Handle)   \n",
        "        # loss = criterion(predictions, batch.Party)             \n",
        "        loss.backward()       \n",
        "        optimizer.step()      \n",
        "        \n",
        "        epoch_loss += loss.item()  \n",
        "\n",
        "        if iter_count % print_every == 0:\n",
        "            print('%d %.4f' % (iter_count, loss))\n",
        "        iter_count+=1\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QBBed2k5P81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    #no dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    #no autograd\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            #retrieve text \n",
        "            text, text_lengths = batch.Tweet   \n",
        "            \n",
        "                    \n",
        "            if text.size(1) < 5:\n",
        "              continue \n",
        "\n",
        "            predictions = model(text, text_lengths)\n",
        "            \n",
        "             #CLTR VS ALL\n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.Handle)   \n",
        "            # loss = criterion(predictions, batch.Party)  \n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkx3h5-S61lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_validate(model, train_iterator, valid_iterator, optimizer, criterion, n_epochs):\n",
        "  N_EPOCHS = n_epochs\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "      \n",
        "      train_loss = train(model, train_iterator, optimizer, criterion)\n",
        "      valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "      \n",
        "      #save the best model\n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "      \n",
        "      print(f'\\t Train Loss: {train_loss:.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Yx8nSNcEQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import sklearn.metrics\n",
        "\n",
        "def test_scores(model, test_iterator):\n",
        "  # correct = 0\n",
        "  # total = 0\n",
        "  with torch.no_grad():\n",
        "    #there should be only one batch\n",
        "      for batch in test_iterator:\n",
        "        text, text_lengths = batch.Tweet\n",
        "        \n",
        "        #CLTR VS ALL\n",
        "        labels = batch.Handle\n",
        "        # labels = batch.Party\n",
        "\n",
        "        if text.size(1) < 5:\n",
        "          continue \n",
        "        predictions = model(text, text_lengths)\n",
        "        _, pred_classes = torch.max(predictions, 1)\n",
        "\n",
        "        print(text[pred_classes != labels][0])\n",
        "\n",
        "\n",
        "\n",
        "        # for val in range(200):\n",
        "                  \n",
        "        #   w2id = TEXT.vocab.stoi\n",
        "        #   id2w = dict([(value, key) for key, value in w2id.items()])\n",
        "        #   textttt = [id2w[i.item()] for i in text[pred_classes != labels][val]] \n",
        "        #   if textttt[0]!=\"\":\n",
        "        #     print(\" \".join(textttt))\n",
        "        #     print(labels[pred_classes != labels][val])\n",
        "        #     # print(labels[pred_classes != labels][val])\n",
        "\n",
        "\n",
        "        # total += labels.size(0)\n",
        "        # correct += (pred_classes == labels).sum().item()\n",
        "\n",
        "        print(\"ACCURACY\", sklearn.metrics.accuracy_score(labels.cpu().numpy(),  pred_classes.cpu().numpy()).round(3))\n",
        "        print(\"F1 SCORE\", sklearn.metrics.f1_score(labels.cpu().numpy(),  pred_classes.cpu().numpy(), average='micro').round(3))\n",
        "        print(\"PRECISION\", sklearn.metrics.precision_score(labels.cpu().numpy(),  pred_classes.cpu().numpy(), average='micro').round(3))\n",
        "        print(\"RECALL\", sklearn.metrics.recall_score(labels.cpu().numpy(),  pred_classes.cpu().numpy(), average='micro').round(3))\n",
        "\n",
        "        # return sklearn.metrics.f1_score(labels.cpu().numpy(),  pred_classes.cpu().numpy()).round(3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM49ra8tVX75",
        "colab_type": "code",
        "outputId": "10a65aad-1a58-4267-d902-75ff96f034f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "CNN_EMBED_DIM = 100\n",
        "# CNN_HIDDEN_DIM = 100\n",
        "CNN_NUM_CONV_LAYERS = 3\n",
        "CNN_DROPOUT = 0.3\n",
        "CNN_LEARNING_RATE = 0.01\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "f1s = []\n",
        "\n",
        "for i in [5, 10, 20, 50, 100, 500]:\n",
        "  CNN_HIDDEN_DIM = i \n",
        "  cnn_model = CNN(embs_vocab, CNN_EMBED_DIM, CNN_HIDDEN_DIM, CNN_NUM_CONV_LAYERS, NUM_CATEGORIES, dropout=CNN_DROPOUT).to(device)\n",
        "  cnn_optimizer = optim.Adam(cnn_model.parameters(), lr = CNN_LEARNING_RATE)\n",
        "  cnn_criterion = nn.CrossEntropyLoss()\n",
        "  train_and_validate(cnn_model, train_iterator, valid_iterator, cnn_optimizer, cnn_criterion, NUM_EPOCHS)\n",
        "  f1 = test_scores(cnn_model, test_iterator)\n",
        "  f1s.append(f1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 6.1810\n",
            "50 6.0503\n",
            "100 6.1129\n",
            "150 6.0910\n",
            "200 6.1267\n",
            "250 5.9344\n",
            "300 6.0949\n",
            "350 6.0962\n",
            "400 6.0968\n",
            "450 6.0233\n",
            "500 6.0735\n",
            "550 6.0560\n",
            "600 6.0731\n",
            "650 6.0864\n",
            "700 6.0611\n",
            "750 6.0931\n",
            "800 6.0794\n",
            "850 6.0969\n",
            "900 6.1172\n",
            "\t Train Loss: 6.051\n",
            "\t Val. Loss: 6.019\n",
            "0 6.0600\n",
            "50 6.0806\n",
            "100 6.0898\n",
            "150 6.0992\n",
            "200 6.0696\n",
            "250 6.0739\n",
            "300 5.9768\n",
            "350 6.0679\n",
            "400 6.0328\n",
            "450 6.0617\n",
            "500 6.0866\n",
            "550 6.0826\n",
            "600 6.0420\n",
            "650 6.0791\n",
            "700 6.0807\n",
            "750 6.0695\n",
            "800 6.0738\n",
            "850 6.0771\n",
            "900 5.9848\n",
            "\t Train Loss: 6.042\n",
            "\t Val. Loss: 6.018\n",
            "0 6.0663\n",
            "50 6.0612\n",
            "100 6.0531\n",
            "150 6.0764\n",
            "200 6.0778\n",
            "250 6.0743\n",
            "300 6.0747\n",
            "350 6.0849\n",
            "400 6.0480\n",
            "450 6.0852\n",
            "500 6.0855\n",
            "550 6.0969\n",
            "600 6.0741\n",
            "650 6.0278\n",
            "700 6.0539\n",
            "750 6.0790\n",
            "800 6.1457\n",
            "850 6.0390\n",
            "900 6.1024\n",
            "\t Train Loss: 6.034\n",
            "\t Val. Loss: 6.017\n",
            "0 6.0216\n",
            "50 6.0763\n",
            "100 6.0665\n",
            "150 6.0085\n",
            "200 6.0143\n",
            "250 6.0695\n",
            "300 6.0072\n",
            "350 6.0247\n",
            "400 6.0580\n",
            "450 6.0009\n",
            "500 6.0226\n",
            "550 6.0951\n",
            "600 6.0780\n",
            "650 6.0697\n",
            "700 6.0869\n",
            "750 6.0667\n",
            "800 6.0779\n",
            "850 6.0959\n",
            "900 6.1777\n",
            "\t Train Loss: 6.032\n",
            "\t Val. Loss: 6.017\n",
            "0 6.0663\n",
            "50 6.0287\n",
            "100 5.9255\n",
            "150 6.0840\n",
            "200 6.0673\n",
            "250 6.0835\n",
            "300 6.0531\n",
            "350 6.0630\n",
            "400 6.0950\n",
            "450 6.0860\n",
            "500 6.0484\n",
            "550 6.0717\n",
            "600 6.1042\n",
            "650 6.0818\n",
            "700 6.1101\n",
            "750 6.0840\n",
            "800 6.0713\n",
            "850 6.0875\n",
            "900 6.0158\n",
            "\t Train Loss: 6.022\n",
            "\t Val. Loss: 6.017\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.005\n",
            "F1 SCORE 0.005\n",
            "PRECISION 0.005\n",
            "RECALL 0.005\n",
            "0 6.0697\n",
            "50 6.1032\n",
            "100 6.0860\n",
            "150 6.0600\n",
            "200 6.0836\n",
            "250 6.0836\n",
            "300 6.1124\n",
            "350 6.0694\n",
            "400 6.0652\n",
            "450 6.0679\n",
            "500 6.0850\n",
            "550 6.0577\n",
            "600 6.0959\n",
            "650 6.1059\n",
            "700 6.0628\n",
            "750 6.0743\n",
            "800 5.9669\n",
            "850 6.0821\n",
            "900 6.0758\n",
            "\t Train Loss: 6.052\n",
            "\t Val. Loss: 6.018\n",
            "0 6.0941\n",
            "50 6.0742\n",
            "100 6.0920\n",
            "150 6.1024\n",
            "200 6.0594\n",
            "250 6.0845\n",
            "300 6.1007\n",
            "350 6.0350\n",
            "400 6.0720\n",
            "450 6.0920\n",
            "500 6.0804\n",
            "550 6.0474\n",
            "600 6.0933\n",
            "650 6.0510\n",
            "700 6.0646\n",
            "750 6.0704\n",
            "800 6.1048\n",
            "850 6.0878\n",
            "900 6.1053\n",
            "\t Train Loss: 6.040\n",
            "\t Val. Loss: 6.017\n",
            "0 6.0745\n",
            "50 6.0526\n",
            "100 6.0244\n",
            "150 6.0611\n",
            "200 6.0681\n",
            "250 6.0804\n",
            "300 6.0753\n",
            "350 5.8539\n",
            "400 6.0651\n",
            "450 6.1091\n",
            "500 6.0846\n",
            "550 5.9981\n",
            "600 6.0742\n",
            "650 6.1093\n",
            "700 6.0787\n",
            "750 6.0089\n",
            "800 6.0819\n",
            "850 6.0677\n",
            "900 6.0595\n",
            "\t Train Loss: 6.039\n",
            "\t Val. Loss: 6.023\n",
            "0 6.0832\n",
            "50 5.9604\n",
            "100 6.0431\n",
            "150 6.0402\n",
            "200 6.0627\n",
            "250 6.1084\n",
            "300 6.0444\n",
            "350 6.0657\n",
            "400 6.0630\n",
            "450 6.0608\n",
            "500 6.0998\n",
            "550 6.0743\n",
            "600 6.0811\n",
            "650 6.0588\n",
            "700 6.1009\n",
            "750 6.0657\n",
            "800 6.0874\n",
            "850 6.1076\n",
            "900 6.0780\n",
            "\t Train Loss: 6.022\n",
            "\t Val. Loss: 6.023\n",
            "0 5.9920\n",
            "50 6.0625\n",
            "100 6.0929\n",
            "150 6.0130\n",
            "200 6.0653\n",
            "250 6.0776\n",
            "300 6.1168\n",
            "350 6.0910\n",
            "400 5.9826\n",
            "450 6.0766\n",
            "500 6.0914\n",
            "550 6.0997\n",
            "600 6.0370\n",
            "650 6.0579\n",
            "700 5.9763\n",
            "750 6.1112\n",
            "800 6.0824\n",
            "850 6.0364\n",
            "900 6.0732\n",
            "\t Train Loss: 6.031\n",
            "\t Val. Loss: 6.029\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.005\n",
            "F1 SCORE 0.005\n",
            "PRECISION 0.005\n",
            "RECALL 0.005\n",
            "0 6.0860\n",
            "50 6.0714\n",
            "100 6.1120\n",
            "150 6.0697\n",
            "200 6.0717\n",
            "250 6.0890\n",
            "300 6.0725\n",
            "350 6.0897\n",
            "400 6.1023\n",
            "450 6.0104\n",
            "500 6.0923\n",
            "550 6.0661\n",
            "600 6.1085\n",
            "650 6.2898\n",
            "700 6.0652\n",
            "750 6.0850\n",
            "800 6.0977\n",
            "850 6.0640\n",
            "900 6.0545\n",
            "\t Train Loss: 6.048\n",
            "\t Val. Loss: 6.019\n",
            "0 6.0813\n",
            "50 6.1009\n",
            "100 6.0626\n",
            "150 5.9517\n",
            "200 6.0856\n",
            "250 6.0625\n",
            "300 6.1180\n",
            "350 6.1610\n",
            "400 6.0693\n",
            "450 6.0652\n",
            "500 6.1174\n",
            "550 6.0797\n",
            "600 6.0617\n",
            "650 6.0763\n",
            "700 6.0863\n",
            "750 6.0744\n",
            "800 6.0920\n",
            "850 6.0871\n",
            "900 6.0772\n",
            "\t Train Loss: 6.047\n",
            "\t Val. Loss: 6.025\n",
            "0 6.0484\n",
            "50 6.0617\n",
            "100 5.9792\n",
            "150 6.0511\n",
            "200 6.0996\n",
            "250 6.1389\n",
            "300 6.0752\n",
            "350 6.0733\n",
            "400 6.0567\n",
            "450 5.9638\n",
            "500 6.0794\n",
            "550 6.2456\n",
            "600 6.0639\n",
            "650 6.0142\n",
            "700 6.0877\n",
            "750 6.1068\n",
            "800 6.0273\n",
            "850 6.1112\n",
            "900 6.1077\n",
            "\t Train Loss: 6.036\n",
            "\t Val. Loss: 6.030\n",
            "0 6.0170\n",
            "50 6.0723\n",
            "100 6.0451\n",
            "150 6.0699\n",
            "200 6.0790\n",
            "250 6.1058\n",
            "300 5.9272\n",
            "350 6.1071\n",
            "400 6.1495\n",
            "450 6.1050\n",
            "500 5.9258\n",
            "550 6.0835\n",
            "600 6.0868\n",
            "650 6.0662\n",
            "700 6.0452\n",
            "750 5.9689\n",
            "800 6.0672\n",
            "850 6.0523\n",
            "900 6.0458\n",
            "\t Train Loss: 6.012\n",
            "\t Val. Loss: 6.034\n",
            "0 6.0718\n",
            "50 6.0170\n",
            "100 6.0360\n",
            "150 6.0664\n",
            "200 6.0713\n",
            "250 5.9835\n",
            "300 6.0808\n",
            "350 6.0728\n",
            "400 6.0552\n",
            "450 6.0608\n",
            "500 6.0322\n",
            "550 5.9826\n",
            "600 6.0122\n",
            "650 6.0014\n",
            "700 5.9865\n",
            "750 6.1130\n",
            "800 6.0921\n",
            "850 6.0086\n",
            "900 6.0610\n",
            "\t Train Loss: 6.022\n",
            "\t Val. Loss: 6.043\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.005\n",
            "F1 SCORE 0.005\n",
            "PRECISION 0.005\n",
            "RECALL 0.005\n",
            "0 6.0652\n",
            "50 6.0921\n",
            "100 6.0701\n",
            "150 6.0812\n",
            "200 6.1050\n",
            "250 5.9968\n",
            "300 6.1002\n",
            "350 6.1162\n",
            "400 5.9870\n",
            "450 6.0837\n",
            "500 6.0450\n",
            "550 6.0924\n",
            "600 6.1601\n",
            "650 6.1039\n",
            "700 6.0891\n",
            "750 6.1689\n",
            "800 6.0201\n",
            "850 6.0917\n",
            "900 6.0878\n",
            "\t Train Loss: 6.055\n",
            "\t Val. Loss: 6.030\n",
            "0 6.0914\n",
            "50 6.0492\n",
            "100 6.0307\n",
            "150 6.1369\n",
            "200 6.1157\n",
            "250 6.0026\n",
            "300 6.1133\n",
            "350 6.0551\n",
            "400 6.0956\n",
            "450 6.0476\n",
            "500 6.0863\n",
            "550 6.0836\n",
            "600 6.0892\n",
            "650 6.0699\n",
            "700 6.0730\n",
            "750 6.1225\n",
            "800 6.0655\n",
            "850 6.1211\n",
            "900 5.9986\n",
            "\t Train Loss: 6.045\n",
            "\t Val. Loss: 6.025\n",
            "0 6.0170\n",
            "50 6.1033\n",
            "100 6.1356\n",
            "150 5.9845\n",
            "200 6.0685\n",
            "250 6.0212\n",
            "300 6.0937\n",
            "350 6.0529\n",
            "400 6.0789\n",
            "450 6.1211\n",
            "500 6.0912\n",
            "550 6.0059\n",
            "600 5.9758\n",
            "650 6.2939\n",
            "700 6.0745\n",
            "750 6.0859\n",
            "800 5.9475\n",
            "850 6.0698\n",
            "900 6.1856\n",
            "\t Train Loss: 6.046\n",
            "\t Val. Loss: 6.034\n",
            "0 5.9409\n",
            "50 6.1706\n",
            "100 6.1661\n",
            "150 6.0321\n",
            "200 6.1595\n",
            "250 5.9776\n",
            "300 6.0508\n",
            "350 6.1756\n",
            "400 6.0762\n",
            "450 6.0930\n",
            "500 6.0722\n",
            "550 6.0919\n",
            "600 6.0142\n",
            "650 6.0499\n",
            "700 6.2143\n",
            "750 6.1044\n",
            "800 6.1055\n",
            "850 6.0858\n",
            "900 6.5521\n",
            "\t Train Loss: 6.027\n",
            "\t Val. Loss: 6.039\n",
            "0 6.1132\n",
            "50 6.0660\n",
            "100 6.0643\n",
            "150 6.0758\n",
            "200 6.0258\n",
            "250 6.0110\n",
            "300 6.0507\n",
            "350 6.0478\n",
            "400 5.9905\n",
            "450 6.0188\n",
            "500 5.9602\n",
            "550 6.0859\n",
            "600 6.1031\n",
            "650 5.9876\n",
            "700 5.9009\n",
            "750 6.0565\n",
            "800 5.9655\n",
            "850 5.9827\n",
            "900 6.0167\n",
            "\t Train Loss: 6.008\n",
            "\t Val. Loss: 6.073\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.005\n",
            "F1 SCORE 0.005\n",
            "PRECISION 0.005\n",
            "RECALL 0.005\n",
            "0 6.1348\n",
            "50 6.0807\n",
            "100 6.1780\n",
            "150 6.3107\n",
            "200 6.1112\n",
            "250 6.1098\n",
            "300 6.0721\n",
            "350 6.0798\n",
            "400 6.0841\n",
            "450 6.1349\n",
            "500 6.1925\n",
            "550 6.0986\n",
            "600 6.1309\n",
            "650 6.0717\n",
            "700 6.0558\n",
            "750 6.1109\n",
            "800 6.0814\n",
            "850 6.0619\n",
            "900 6.0935\n",
            "\t Train Loss: 6.078\n",
            "\t Val. Loss: 6.037\n",
            "0 6.0376\n",
            "50 6.1278\n",
            "100 6.0362\n",
            "150 6.0931\n",
            "200 6.1217\n",
            "250 6.0592\n",
            "300 6.1292\n",
            "350 6.0232\n",
            "400 6.0752\n",
            "450 6.0485\n",
            "500 6.0379\n",
            "550 6.1413\n",
            "600 6.0814\n",
            "650 6.1739\n",
            "700 6.0812\n",
            "750 6.1006\n",
            "800 6.0561\n",
            "850 6.1230\n",
            "900 6.0838\n",
            "\t Train Loss: 6.063\n",
            "\t Val. Loss: 6.069\n",
            "0 6.0170\n",
            "50 6.1984\n",
            "100 6.0516\n",
            "150 6.0166\n",
            "200 6.0631\n",
            "250 6.0274\n",
            "300 6.0281\n",
            "350 6.0184\n",
            "400 6.0887\n",
            "450 6.0697\n",
            "500 6.0885\n",
            "550 6.0184\n",
            "600 6.0788\n",
            "650 6.0856\n",
            "700 6.0936\n",
            "750 5.8305\n",
            "800 6.1064\n",
            "850 6.0278\n",
            "900 6.1972\n",
            "\t Train Loss: 6.043\n",
            "\t Val. Loss: 6.065\n",
            "0 6.0216\n",
            "50 6.0997\n",
            "100 6.1225\n",
            "150 6.0798\n",
            "200 6.0329\n",
            "250 6.0385\n",
            "300 6.0272\n",
            "350 6.0588\n",
            "400 6.0395\n",
            "450 6.1238\n",
            "500 6.1232\n",
            "550 5.9133\n",
            "600 5.9331\n",
            "650 6.0514\n",
            "700 6.0820\n",
            "750 6.0905\n",
            "800 6.0927\n",
            "850 6.0610\n",
            "900 5.9991\n",
            "\t Train Loss: 6.036\n",
            "\t Val. Loss: 6.080\n",
            "0 5.9833\n",
            "50 5.7635\n",
            "100 6.1669\n",
            "150 5.9473\n",
            "200 6.0775\n",
            "250 5.9705\n",
            "300 6.0220\n",
            "350 6.1920\n",
            "400 6.0843\n",
            "450 6.1077\n",
            "500 6.0587\n",
            "550 6.1075\n",
            "600 6.0285\n",
            "650 6.0285\n",
            "700 5.9831\n",
            "750 6.0671\n",
            "800 6.0365\n",
            "850 6.0571\n",
            "900 6.0737\n",
            "\t Train Loss: 6.032\n",
            "\t Val. Loss: 6.078\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.004\n",
            "F1 SCORE 0.004\n",
            "PRECISION 0.004\n",
            "RECALL 0.004\n",
            "0 6.2067\n",
            "50 6.0696\n",
            "100 6.2152\n",
            "150 6.0977\n",
            "200 6.2756\n",
            "250 6.4818\n",
            "300 6.1409\n",
            "350 7.0206\n",
            "400 6.4888\n",
            "450 6.4601\n",
            "500 6.1916\n",
            "550 6.1336\n",
            "600 6.3986\n",
            "650 6.1987\n",
            "700 6.6966\n",
            "750 6.1306\n",
            "800 6.3525\n",
            "850 6.0513\n",
            "900 6.0873\n",
            "\t Train Loss: 6.281\n",
            "\t Val. Loss: 6.265\n",
            "0 6.2120\n",
            "50 6.9697\n",
            "100 6.5148\n",
            "150 5.8997\n",
            "200 6.1286\n",
            "250 6.5352\n",
            "300 6.1250\n",
            "350 6.2802\n",
            "400 6.2175\n",
            "450 6.3025\n",
            "500 6.2805\n",
            "550 6.1944\n",
            "600 6.1875\n",
            "650 6.0981\n",
            "700 6.6495\n",
            "750 6.2006\n",
            "800 6.1742\n",
            "850 6.1344\n",
            "900 6.7128\n",
            "\t Train Loss: 6.224\n",
            "\t Val. Loss: 6.297\n",
            "0 6.0678\n",
            "50 6.1410\n",
            "100 6.0422\n",
            "150 6.1638\n",
            "200 6.1248\n",
            "250 6.0207\n",
            "300 5.9136\n",
            "350 5.9609\n",
            "400 5.9723\n",
            "450 6.0999\n",
            "500 6.0791\n",
            "550 6.5171\n",
            "600 6.0483\n",
            "650 6.2453\n",
            "700 6.0562\n",
            "750 6.9773\n",
            "800 6.1553\n",
            "850 6.2984\n",
            "900 6.2715\n",
            "\t Train Loss: 6.192\n",
            "\t Val. Loss: 6.278\n",
            "0 5.8886\n",
            "50 6.2933\n",
            "100 5.8902\n",
            "150 7.9665\n",
            "200 6.2519\n",
            "250 6.1972\n",
            "300 6.1893\n",
            "350 6.1121\n",
            "400 5.8455\n",
            "450 6.1445\n",
            "500 6.2487\n",
            "550 6.0719\n",
            "600 6.0260\n",
            "650 5.9460\n",
            "700 6.4077\n",
            "750 6.2345\n",
            "800 9.0188\n",
            "850 6.0646\n",
            "900 6.0194\n",
            "\t Train Loss: 6.161\n",
            "\t Val. Loss: 6.379\n",
            "0 5.8701\n",
            "50 5.6191\n",
            "100 6.0384\n",
            "150 5.8196\n",
            "200 6.4973\n",
            "250 6.0017\n",
            "300 6.0113\n",
            "350 6.2272\n",
            "400 6.0758\n",
            "450 6.0925\n",
            "500 5.8943\n",
            "550 5.8828\n",
            "600 6.1271\n",
            "650 5.9093\n",
            "700 6.0629\n",
            "750 5.9860\n",
            "800 6.0455\n",
            "850 6.9454\n",
            "900 6.0938\n",
            "\t Train Loss: 6.169\n",
            "\t Val. Loss: 6.451\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.007\n",
            "F1 SCORE 0.007\n",
            "PRECISION 0.007\n",
            "RECALL 0.007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbIEKi9YDRcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_scores(cnn_model, test_iterator)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYHBIasg9DVg",
        "colab_type": "code",
        "outputId": "179f224c-6367-44b5-cd50-fc83f3dc3e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f1s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.584, 0.791, 0.69, 0.817, 0.786, 0.675]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vIQzs9J5eej",
        "colab_type": "text"
      },
      "source": [
        "#Bi-Directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYC_qtQ48Ug2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiDiLSTM(nn.Module):\n",
        "  def __init__(self, embs_vocab, embed_size, hidden_size, num_layers, num_categories, bidirectional=True, dropout=0.):\n",
        "    super(BiDiLSTM, self).__init__()\n",
        "\n",
        "    vocab_size = len(embs_vocab)\n",
        "    embed_size = embed_size #TODO\n",
        "    hidden_size = hidden_size\n",
        "    num_layers = num_layers\n",
        "    num_categories = num_categories \n",
        "    self.bidirectional = bidirectional\n",
        "\n",
        "    self.embed = nn.Embedding.from_pretrained(embs_vocab)\n",
        "  \n",
        "    #lstm layer\n",
        "    self.lstm = nn.LSTM(embed_size, \n",
        "                        hidden_size, \n",
        "                        num_layers=num_layers, \n",
        "                        bidirectional=self.bidirectional, \n",
        "                        dropout=dropout,\n",
        "                        batch_first=True)\n",
        "    \n",
        "    #dense layer\n",
        "    if self.bidirectional:\n",
        "      self.fc = nn.Linear(hidden_size * 2, num_categories)\n",
        "    else:\n",
        "      self.fc = nn.Linear(hidden_size, num_categories)\n",
        "\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "    embedded = self.embed(text) #[batch size, sent_len, emb dim]\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
        "    \n",
        "    output, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "    #hidden = [batch size, num layers * num directions,hid dim]\n",
        "    #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "    #concat the final forward and backward hidden state\n",
        "    if self.bidirectional:\n",
        "      hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "    else:\n",
        "      hidden = hidden.squeeze()   \n",
        "       \n",
        "    #hidden = [batch size, hid dim * num directions]\n",
        "    logits = self.fc(hidden)\n",
        "\n",
        "    return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIulAb8m_pFG",
        "colab_type": "code",
        "outputId": "3f93328d-96cb-42f7-e107-143d84559902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "LSTM_EMBED_DIM = 100\n",
        "# LSTM_HIDDEN_DIM = 82\n",
        "LSTM_NUM_LAYERS = 1\n",
        "LSTM_DROPOUT = 0.3\n",
        "LSTM_LEARNING_RATE = 0.001\n",
        "BIDIRECTIONAL = True\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "\n",
        "f1s = []\n",
        "\n",
        "for i in [20, 50]:\n",
        "  LSTM_HIDDEN_DIM = i\n",
        "  lstm_model = BiDiLSTM(embs_vocab, LSTM_EMBED_DIM, LSTM_HIDDEN_DIM, LSTM_NUM_LAYERS, NUM_CATEGORIES,bidirectional = BIDIRECTIONAL, dropout=LSTM_DROPOUT).to(device)\n",
        "  lstm_optimizer = optim.Adam(lstm_model.parameters(), lr = LSTM_LEARNING_RATE)\n",
        "  lstm_criterion = nn.CrossEntropyLoss()\n",
        "  train_and_validate(lstm_model, train_iterator, valid_iterator, lstm_optimizer, lstm_criterion, NUM_EPOCHS)\n",
        "  f1 = test_scores(lstm_model, test_iterator)\n",
        "  f1s.append(f1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 6.0689\n",
            "50 6.0810\n",
            "100 6.0757\n",
            "150 6.1169\n",
            "200 6.0717\n",
            "250 6.0705\n",
            "300 6.0908\n",
            "350 6.1111\n",
            "400 6.0430\n",
            "450 6.0859\n",
            "500 6.0890\n",
            "550 6.0412\n",
            "600 6.0568\n",
            "650 6.0617\n",
            "700 6.0534\n",
            "750 6.0373\n",
            "800 5.9706\n",
            "850 6.0589\n",
            "900 6.0171\n",
            "\t Train Loss: 6.034\n",
            "\t Val. Loss: 5.951\n",
            "0 5.9424\n",
            "50 5.9712\n",
            "100 5.9770\n",
            "150 5.9818\n",
            "200 5.9504\n",
            "250 5.9186\n",
            "300 5.8887\n",
            "350 5.9311\n",
            "400 5.9266\n",
            "450 6.0305\n",
            "500 5.9351\n",
            "550 5.8606\n",
            "600 6.0521\n",
            "650 5.8832\n",
            "700 5.9816\n",
            "750 5.7408\n",
            "800 5.8780\n",
            "850 5.6843\n",
            "900 5.8163\n",
            "\t Train Loss: 5.916\n",
            "\t Val. Loss: 5.852\n",
            "0 5.9023\n",
            "50 5.7350\n",
            "100 5.7511\n",
            "150 5.8642\n",
            "200 5.8907\n",
            "250 5.8067\n",
            "300 5.7352\n",
            "350 5.9266\n",
            "400 5.8627\n",
            "450 5.8294\n",
            "500 5.8246\n",
            "550 5.9100\n",
            "600 5.8019\n",
            "650 5.7151\n",
            "700 5.8113\n",
            "750 5.8957\n",
            "800 5.7361\n",
            "850 5.8704\n",
            "900 5.8259\n",
            "\t Train Loss: 5.822\n",
            "\t Val. Loss: 5.807\n",
            "0 5.8458\n",
            "50 5.8402\n",
            "100 5.8816\n",
            "150 5.6911\n",
            "200 5.8689\n",
            "250 5.7495\n",
            "300 5.7487\n",
            "350 5.9274\n",
            "400 5.8427\n",
            "450 5.5253\n",
            "500 5.8941\n",
            "550 5.9437\n",
            "600 5.7311\n",
            "650 5.9323\n",
            "700 5.7805\n",
            "750 5.7906\n",
            "800 5.8546\n",
            "850 5.8625\n",
            "900 5.9017\n",
            "\t Train Loss: 5.770\n",
            "\t Val. Loss: 5.782\n",
            "0 5.8129\n",
            "50 5.4373\n",
            "100 5.9774\n",
            "150 5.8546\n",
            "200 5.7657\n",
            "250 5.6082\n",
            "300 5.7498\n",
            "350 5.7075\n",
            "400 5.7979\n",
            "450 5.8147\n",
            "500 5.8162\n",
            "550 5.7337\n",
            "600 5.7836\n",
            "650 5.5908\n",
            "700 5.7282\n",
            "750 5.7175\n",
            "800 5.6116\n",
            "850 5.8250\n",
            "900 5.8715\n",
            "\t Train Loss: 5.749\n",
            "\t Val. Loss: 5.765\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.019\n",
            "F1 SCORE 0.019\n",
            "PRECISION 0.019\n",
            "RECALL 0.019\n",
            "0 6.0709\n",
            "50 6.0836\n",
            "100 6.0999\n",
            "150 6.0992\n",
            "200 6.0913\n",
            "250 6.0707\n",
            "300 6.0762\n",
            "350 6.0369\n",
            "400 6.0026\n",
            "450 5.9924\n",
            "500 5.9650\n",
            "550 6.0000\n",
            "600 5.9964\n",
            "650 5.9838\n",
            "700 6.0291\n",
            "750 5.9732\n",
            "800 6.0023\n",
            "850 5.9127\n",
            "900 5.9355\n",
            "\t Train Loss: 5.974\n",
            "\t Val. Loss: 5.879\n",
            "0 5.8992\n",
            "50 5.8349\n",
            "100 5.8974\n",
            "150 5.9356\n",
            "200 5.9688\n",
            "250 5.9224\n",
            "300 5.8788\n",
            "350 5.8992\n",
            "400 5.9601\n",
            "450 5.8499\n",
            "500 5.9065\n",
            "550 5.7520\n",
            "600 5.8375\n",
            "650 5.7671\n",
            "700 5.7775\n",
            "750 5.9404\n",
            "800 5.6671\n",
            "850 5.7808\n",
            "900 5.8671\n",
            "\t Train Loss: 5.841\n",
            "\t Val. Loss: 5.800\n",
            "0 5.8223\n",
            "50 5.7704\n",
            "100 5.8619\n",
            "150 5.7505\n",
            "200 5.7128\n",
            "250 5.7043\n",
            "300 5.7319\n",
            "350 5.9342\n",
            "400 5.7407\n",
            "450 5.8049\n",
            "500 5.7730\n",
            "550 5.6438\n",
            "600 5.8061\n",
            "650 5.6632\n",
            "700 5.6559\n",
            "750 5.9050\n",
            "800 5.9204\n",
            "850 5.8675\n",
            "900 5.5829\n",
            "\t Train Loss: 5.745\n",
            "\t Val. Loss: 5.737\n",
            "0 5.6063\n",
            "50 5.8166\n",
            "100 5.6352\n",
            "150 5.4553\n",
            "200 5.6114\n",
            "250 5.7213\n",
            "300 5.6361\n",
            "350 5.5413\n",
            "400 5.6183\n",
            "450 5.7844\n",
            "500 5.8743\n",
            "550 5.3231\n",
            "600 5.7182\n",
            "650 5.7676\n",
            "700 5.7901\n",
            "750 5.6597\n",
            "800 5.7699\n",
            "850 5.6967\n",
            "900 5.8215\n",
            "\t Train Loss: 5.686\n",
            "\t Val. Loss: 5.692\n",
            "0 5.5959\n",
            "50 5.6536\n",
            "100 5.7411\n",
            "150 5.6665\n",
            "200 5.8835\n",
            "250 5.4471\n",
            "300 5.8929\n",
            "350 5.5458\n",
            "400 5.7142\n",
            "450 5.6663\n",
            "500 5.7452\n",
            "550 5.6709\n",
            "600 5.5363\n",
            "650 5.6915\n",
            "700 5.8069\n",
            "750 5.5523\n",
            "800 5.7105\n",
            "850 5.5042\n",
            "900 5.6653\n",
            "\t Train Loss: 5.640\n",
            "\t Val. Loss: 5.659\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.028\n",
            "F1 SCORE 0.028\n",
            "PRECISION 0.028\n",
            "RECALL 0.028\n",
            "0 6.1016\n",
            "50 6.0560\n",
            "100 6.0619\n",
            "150 6.0861\n",
            "200 6.0647\n",
            "250 6.0493\n",
            "300 6.0333\n",
            "350 6.0263\n",
            "400 5.9945\n",
            "450 5.9429\n",
            "500 5.9011\n",
            "550 5.9751\n",
            "600 5.9294\n",
            "650 5.7614\n",
            "700 5.8380\n",
            "750 5.8697\n",
            "800 5.7672\n",
            "850 5.8966\n",
            "900 5.8146\n",
            "\t Train Loss: 5.933\n",
            "\t Val. Loss: 5.803\n",
            "0 5.7665\n",
            "50 5.8460\n",
            "100 5.9171\n",
            "150 5.7029\n",
            "200 5.7506\n",
            "250 5.8206\n",
            "300 5.7784\n",
            "350 5.8977\n",
            "400 5.8183\n",
            "450 5.7199\n",
            "500 5.8018\n",
            "550 5.8048\n",
            "600 5.5946\n",
            "650 5.6027\n",
            "700 5.8803\n",
            "750 5.7210\n",
            "800 5.7530\n",
            "850 5.6991\n",
            "900 5.8600\n",
            "\t Train Loss: 5.716\n",
            "\t Val. Loss: 5.676\n",
            "0 5.6677\n",
            "50 5.6509\n",
            "100 5.6710\n",
            "150 5.6955\n",
            "200 5.8259\n",
            "250 5.5680\n",
            "300 5.6046\n",
            "350 5.5098\n",
            "400 5.5395\n",
            "450 5.7360\n",
            "500 5.7900\n",
            "550 5.6810\n",
            "600 5.7313\n",
            "650 5.7024\n",
            "700 5.7118\n",
            "750 5.5230\n",
            "800 5.6314\n",
            "850 5.7079\n",
            "900 5.6883\n",
            "\t Train Loss: 5.602\n",
            "\t Val. Loss: 5.614\n",
            "0 5.6131\n",
            "50 5.7709\n",
            "100 5.6296\n",
            "150 5.4010\n",
            "200 5.6427\n",
            "250 5.6139\n",
            "300 4.7926\n",
            "350 5.4209\n",
            "400 5.4041\n",
            "450 5.5610\n",
            "500 5.5444\n",
            "550 5.6182\n",
            "600 5.4352\n",
            "650 5.7068\n",
            "700 5.4607\n",
            "750 5.7408\n",
            "800 5.7068\n",
            "850 5.5258\n",
            "900 5.6602\n",
            "\t Train Loss: 5.535\n",
            "\t Val. Loss: 5.578\n",
            "0 5.2914\n",
            "50 5.4661\n",
            "100 5.4756\n",
            "150 5.6317\n",
            "200 5.6706\n",
            "250 5.4095\n",
            "300 5.2904\n",
            "350 5.8066\n",
            "400 5.4999\n",
            "450 5.5615\n",
            "500 5.6088\n",
            "550 5.5824\n",
            "600 5.3569\n",
            "650 5.7640\n",
            "700 5.3764\n",
            "750 5.6054\n",
            "800 5.5650\n",
            "850 5.5190\n",
            "900 5.5636\n",
            "\t Train Loss: 5.470\n",
            "\t Val. Loss: 5.534\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.041\n",
            "F1 SCORE 0.041\n",
            "PRECISION 0.041\n",
            "RECALL 0.041\n",
            "0 6.0852\n",
            "50 6.0820\n",
            "100 6.0543\n",
            "150 6.0397\n",
            "200 6.0628\n",
            "250 5.9810\n",
            "300 5.9634\n",
            "350 6.0251\n",
            "400 5.9575\n",
            "450 5.8910\n",
            "500 5.7802\n",
            "550 5.8018\n",
            "600 5.9124\n",
            "650 5.8619\n",
            "700 6.0443\n",
            "750 5.5940\n",
            "800 5.6822\n",
            "850 5.6801\n",
            "900 5.7262\n",
            "\t Train Loss: 5.850\n",
            "\t Val. Loss: 5.710\n",
            "0 5.5999\n",
            "50 5.6462\n",
            "100 5.6694\n",
            "150 5.7950\n",
            "200 5.7206\n",
            "250 5.8145\n",
            "300 5.8450\n",
            "350 5.6377\n",
            "400 5.6615\n",
            "450 5.5775\n",
            "500 5.7649\n",
            "550 5.5824\n",
            "600 5.5341\n",
            "650 5.4998\n",
            "700 5.7507\n",
            "750 5.7178\n",
            "800 5.6264\n",
            "850 5.5821\n",
            "900 5.2589\n",
            "\t Train Loss: 5.609\n",
            "\t Val. Loss: 5.566\n",
            "0 5.5260\n",
            "50 5.4144\n",
            "100 5.3422\n",
            "150 5.5788\n",
            "200 5.4995\n",
            "250 5.4043\n",
            "300 5.4001\n",
            "350 5.5533\n",
            "400 5.5508\n",
            "450 5.7494\n",
            "500 5.3082\n",
            "550 5.5242\n",
            "600 5.4714\n",
            "650 5.2704\n",
            "700 5.5146\n",
            "750 5.2934\n",
            "800 5.3238\n",
            "850 5.5852\n",
            "900 5.7581\n",
            "\t Train Loss: 5.446\n",
            "\t Val. Loss: 5.492\n",
            "0 5.2225\n",
            "50 5.4439\n",
            "100 5.4293\n",
            "150 5.3582\n",
            "200 5.5470\n",
            "250 5.3497\n",
            "300 5.2991\n",
            "350 5.5150\n",
            "400 5.5238\n",
            "450 5.2077\n",
            "500 5.3308\n",
            "550 5.4697\n",
            "600 5.3746\n",
            "650 5.2802\n",
            "700 5.2184\n",
            "750 5.1066\n",
            "800 5.3939\n",
            "850 5.0693\n",
            "900 5.2754\n",
            "\t Train Loss: 5.333\n",
            "\t Val. Loss: 5.437\n",
            "0 5.4743\n",
            "50 5.1704\n",
            "100 5.3664\n",
            "150 5.2144\n",
            "200 5.4539\n",
            "250 5.3111\n",
            "300 5.1663\n",
            "350 5.2041\n",
            "400 5.2371\n",
            "450 5.4201\n",
            "500 5.3218\n",
            "550 5.3401\n",
            "600 4.9840\n",
            "650 5.1319\n",
            "700 5.4227\n",
            "750 5.2695\n",
            "800 5.2702\n",
            "850 5.3629\n",
            "900 5.0881\n",
            "\t Train Loss: 5.235\n",
            "\t Val. Loss: 5.396\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.056\n",
            "F1 SCORE 0.056\n",
            "PRECISION 0.056\n",
            "RECALL 0.056\n",
            "0 6.0680\n",
            "50 6.0717\n",
            "100 6.0674\n",
            "150 5.9722\n",
            "200 6.0470\n",
            "250 5.8507\n",
            "300 5.9368\n",
            "350 5.9543\n",
            "400 5.9862\n",
            "450 5.8456\n",
            "500 5.8828\n",
            "550 5.8970\n",
            "600 5.7402\n",
            "650 5.7217\n",
            "700 5.8642\n",
            "750 5.7543\n",
            "800 5.7625\n",
            "850 5.7013\n",
            "900 5.7616\n",
            "\t Train Loss: 5.839\n",
            "\t Val. Loss: 5.650\n",
            "0 5.6276\n",
            "50 5.6266\n",
            "100 5.5948\n",
            "150 5.8288\n",
            "200 5.5279\n",
            "250 5.6681\n",
            "300 5.6578\n",
            "350 5.6561\n",
            "400 5.7252\n",
            "450 5.3046\n",
            "500 5.3931\n",
            "550 5.4745\n",
            "600 5.4531\n",
            "650 5.4267\n",
            "700 5.5755\n",
            "750 5.4454\n",
            "800 5.7360\n",
            "850 5.3697\n",
            "900 5.5028\n",
            "\t Train Loss: 5.527\n",
            "\t Val. Loss: 5.474\n",
            "0 5.5861\n",
            "50 5.4487\n",
            "100 5.4588\n",
            "150 5.4319\n",
            "200 5.3248\n",
            "250 5.2086\n",
            "300 5.6644\n",
            "350 5.1732\n",
            "400 5.6953\n",
            "450 5.9159\n",
            "500 5.1231\n",
            "550 5.4846\n",
            "600 5.1415\n",
            "650 5.3394\n",
            "700 5.3946\n",
            "750 5.3017\n",
            "800 5.6498\n",
            "850 5.4785\n",
            "900 5.0555\n",
            "\t Train Loss: 5.314\n",
            "\t Val. Loss: 5.382\n",
            "0 5.2247\n",
            "50 5.2252\n",
            "100 5.3878\n",
            "150 5.0724\n",
            "200 5.1045\n",
            "250 5.0694\n",
            "300 5.2054\n",
            "350 5.2450\n",
            "400 5.1750\n",
            "450 5.2448\n",
            "500 5.3777\n",
            "550 4.9131\n",
            "600 4.8427\n",
            "650 5.3688\n",
            "700 5.4645\n",
            "750 5.3301\n",
            "800 5.0020\n",
            "850 5.2336\n",
            "900 5.3393\n",
            "\t Train Loss: 5.154\n",
            "\t Val. Loss: 5.333\n",
            "0 5.3712\n",
            "50 5.1536\n",
            "100 5.1974\n",
            "150 4.9500\n",
            "200 5.0493\n",
            "250 4.8647\n",
            "300 5.0799\n",
            "350 5.3244\n",
            "400 4.6353\n",
            "450 4.9788\n",
            "500 4.6636\n",
            "550 5.1910\n",
            "600 4.8457\n",
            "650 5.1498\n",
            "700 5.1425\n",
            "750 5.1019\n",
            "800 5.0286\n",
            "850 5.3876\n",
            "900 5.1150\n",
            "\t Train Loss: 4.999\n",
            "\t Val. Loss: 5.305\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.067\n",
            "F1 SCORE 0.067\n",
            "PRECISION 0.067\n",
            "RECALL 0.067\n",
            "0 6.0697\n",
            "50 6.0650\n",
            "100 6.0537\n",
            "150 6.0062\n",
            "200 5.8206\n",
            "250 5.9262\n",
            "300 5.8459\n",
            "350 5.8720\n",
            "400 5.8724\n",
            "450 5.8971\n",
            "500 5.8302\n",
            "550 5.8336\n",
            "600 5.7199\n",
            "650 5.5191\n",
            "700 5.7366\n",
            "750 5.7058\n",
            "800 5.7812\n",
            "850 5.6953\n",
            "900 5.5839\n",
            "\t Train Loss: 5.770\n",
            "\t Val. Loss: 5.565\n",
            "0 5.2203\n",
            "50 5.5007\n",
            "100 5.2811\n",
            "150 5.3959\n",
            "200 5.4961\n",
            "250 5.4132\n",
            "300 5.1984\n",
            "350 5.4627\n",
            "400 4.9790\n",
            "450 5.3132\n",
            "500 5.5545\n",
            "550 5.3338\n",
            "600 5.1260\n",
            "650 5.2603\n",
            "700 5.4304\n",
            "750 5.4346\n",
            "800 5.6114\n",
            "850 5.3214\n",
            "900 5.5260\n",
            "\t Train Loss: 5.407\n",
            "\t Val. Loss: 5.368\n",
            "0 5.3028\n",
            "50 5.1696\n",
            "100 5.2467\n",
            "150 4.5630\n",
            "200 5.2190\n",
            "250 5.0546\n",
            "300 4.7990\n",
            "350 5.0050\n",
            "400 4.9679\n",
            "450 5.3811\n",
            "500 4.8844\n",
            "550 4.9046\n",
            "600 4.8864\n",
            "650 5.1803\n",
            "700 5.0779\n",
            "750 5.1943\n",
            "800 5.4194\n",
            "850 5.2274\n",
            "900 5.0787\n",
            "\t Train Loss: 5.072\n",
            "\t Val. Loss: 5.258\n",
            "0 4.9403\n",
            "50 3.7437\n",
            "100 4.8556\n",
            "150 4.7607\n",
            "200 4.1740\n",
            "250 4.7185\n",
            "300 5.0570\n",
            "350 4.9351\n",
            "400 4.6031\n",
            "450 5.2587\n",
            "500 4.5927\n",
            "550 5.2936\n",
            "600 4.7910\n",
            "650 3.8736\n",
            "700 4.4213\n",
            "750 4.7229\n",
            "800 4.9702\n",
            "850 4.1316\n",
            "900 4.7277\n",
            "\t Train Loss: 4.684\n",
            "\t Val. Loss: 5.233\n",
            "0 3.6661\n",
            "50 4.4768\n",
            "100 4.4679\n",
            "150 4.1558\n",
            "200 4.4913\n",
            "250 3.9041\n",
            "300 3.6807\n",
            "350 4.1569\n",
            "400 4.4273\n",
            "450 4.4765\n",
            "500 4.2464\n",
            "550 4.2951\n",
            "600 4.4535\n",
            "650 4.4883\n",
            "700 4.3731\n",
            "750 4.2507\n",
            "800 3.7558\n",
            "850 4.3100\n",
            "900 4.1370\n",
            "\t Train Loss: 4.165\n",
            "\t Val. Loss: 5.332\n",
            "tensor([  776,    86,    26, 20753,     7,     0,    79,   995,    21,  1633,\n",
            "           21,  2291,    21,  1912,    21,  2205,    21,  2958,    21,    15,\n",
            "           21,  1059,    21,   122,    21,  5902,  1082,  1476,   865,    11,\n",
            "           68,   550,  1091,     6,    71,   137,   989,    17,    12,  1340,\n",
            "            4,    30,     3,     0], device='cuda:0')\n",
            "ACCURACY 0.083\n",
            "F1 SCORE 0.083\n",
            "PRECISION 0.083\n",
            "RECALL 0.083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYczHm0BGzLI",
        "colab_type": "code",
        "outputId": "aa1cce96-07c8-4c99-b535-561b360e661a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.63, 0.124, 0.312, 0.152, 0.047, 0.815]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pxsniCUhbWv",
        "colab_type": "code",
        "outputId": "6ca140c6-bac6-4fc6-946e-60de34f6fe80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "test_scores(lstm_model, test_iterator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY 0.841\n",
            "F1 SCORE 0.837\n",
            "PRECISION 0.791\n",
            "RECALL 0.889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4gIV8_CiD01",
        "colab_type": "code",
        "outputId": "9199995b-e7d7-458b-9c13-30e4c4bccf66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#No. of trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The LSTM model has {count_parameters(lstm_model):,} trainable parameters')\n",
        "print(f'The CNN model has {count_parameters(cnn_model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The LSTM model has 2,410,002 trainable parameters\n",
            "The CNN model has 604,502 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb1rR7EQJxem",
        "colab_type": "code",
        "outputId": "f9bd17b6-fe9b-4277-d88e-c8f63ec33b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "w2id = TEXT.vocab.stoi\n",
        "l2id = LABEL.vocab.stoi\n",
        "id2l = dict([(value, key) for key, value in l2id.items()])\n",
        "\n",
        "text = \"Make america great again.\"\n",
        "# \"We need to provide better healthcare\"\n",
        "# \"We need to tax less\"\n",
        "# \"We need to tax more\"\n",
        "\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"spacy\")\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "encoding = torch.tensor([[w2id[token] for token in tokens]]).to(device)\n",
        "length = torch.tensor([encoding.size(1)])\n",
        "\n",
        "predictions = lstm_model(encoding, length)\n",
        "_, pred_classes = torch.max(predictions, 1)\n",
        "\n",
        "\n",
        "\n",
        "cnn_predictions = cnn_model(encoding, length)\n",
        "_, cnn_pred_classes = torch.max(cnn_predictions, 1)\n",
        "\n",
        "print('CNN')\n",
        "print(F.softmax(cnn_predictions))\n",
        "print(text, \" LIKELY COMES FROM \", id2l[cnn_pred_classes.item()])\n",
        "\n",
        "print('LSTM')\n",
        "print(F.softmax(predictions))\n",
        "print(text, \" LIKELY COMES FROM \", id2l[pred_classes.item()])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN\n",
            "tensor([[0.6308, 0.3692]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "Make america great again.  LIKELY COMES FROM  realDonaldTrump\n",
            "LSTM\n",
            "tensor([[0.5658, 0.4342]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "Make america great again.  LIKELY COMES FROM  realDonaldTrump\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmAIxTP6mY07",
        "colab_type": "code",
        "outputId": "fa3b1841-ddcb-4875-aa79-62d2cb71b156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "l2id = LABEL.vocab.stoi\n",
        "l2id"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function torchtext.vocab._default_unk_index>,\n",
              "            {'Democrat': 1, 'Republican': 0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jt2TCGUUBGQ",
        "colab_type": "code",
        "outputId": "7ec989a1-8790-4dd0-f7a1-f6e56bd3d47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "w2id = TEXT.vocab.stoi\n",
        "w2id"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function torchtext.vocab._default_unk_index>,\n",
              "            {'<unk>': 0,\n",
              "             '<pad>': 1,\n",
              "             'the': 2,\n",
              "             '': 3,\n",
              "             'to': 4,\n",
              "             '.': 5,\n",
              "             ',': 6,\n",
              "             '#': 7,\n",
              "             'of': 8,\n",
              "             ':': 9,\n",
              "             'and': 10,\n",
              "             'in': 11,\n",
              "             'a': 12,\n",
              "             'for': 13,\n",
              "             'RT': 14,\n",
              "             'I': 15,\n",
              "             'on': 16,\n",
              "             'is': 17,\n",
              "             '!': 18,\n",
              "             'with': 19,\n",
              "             'our': 20,\n",
              "             '-': 21,\n",
              "             ';': 22,\n",
              "             '&': 23,\n",
              "             'amp': 24,\n",
              "             's': 25,\n",
              "             'that': 26,\n",
              "             'you': 27,\n",
              "             'at': 28,\n",
              "             \"'s\": 29,\n",
              "             'The': 30,\n",
              "             'this': 31,\n",
              "             'are': 32,\n",
              "             'my': 33,\n",
              "             'we': 34,\n",
              "             'from': 35,\n",
              "             'will': 36,\n",
              "             'be': 37,\n",
              "             'have': 38,\n",
              "             'by': 39,\n",
              "             'We': 40,\n",
              "             '\"': 41,\n",
              "             'today': 42,\n",
              "             'about': 43,\n",
              "             'was': 44,\n",
              "             'who': 45,\n",
              "             'their': 46,\n",
              "             'all': 47,\n",
              "             'it': 48,\n",
              "             'an': 49,\n",
              "             'has': 50,\n",
              "             'It': 51,\n",
              "             'This': 52,\n",
              "             'House': 53,\n",
              "             'as': 54,\n",
              "             ' ': 55,\n",
              "             'more': 56,\n",
              "             'Today': 57,\n",
              "             'your': 58,\n",
              "             'not': 59,\n",
              "             '\\n': 60,\n",
              "             'Thank': 61,\n",
              "             'Trump': 62,\n",
              "             'bill': 63,\n",
              "             'can': 64,\n",
              "             'tax': 65,\n",
              "             'Congress': 66,\n",
              "             'work': 67,\n",
              "             '\\n\\n': 68,\n",
              "             'time': 69,\n",
              "             '': 70,\n",
              "             'Act': 71,\n",
              "             'his': 72,\n",
              "             'great': 73,\n",
              "             'out': 74,\n",
              "             '?': 75,\n",
              "             'people': 76,\n",
              "             'President': 77,\n",
              "             'American': 78,\n",
              "             'week': 79,\n",
              "             'up': 80,\n",
              "             '$': 81,\n",
              "             'support': 82,\n",
              "             'Americans': 83,\n",
              "             'do': 84,\n",
              "             'me': 85,\n",
              "             'My': 86,\n",
              "             'Great': 87,\n",
              "             'new': 88,\n",
              "             'students': 89,\n",
              "             'must': 90,\n",
              "             'they': 91,\n",
              "             'one': 92,\n",
              "             '(': 93,\n",
              "             'Thanks': 94,\n",
              "             'help': 95,\n",
              "             ')': 96,\n",
              "             'morning': 97,\n",
              "             'am': 98,\n",
              "             'us': 99,\n",
              "             'day': 100,\n",
              "             'should': 101,\n",
              "             'been': 102,\n",
              "             'now': 103,\n",
              "             'here': 104,\n",
              "             'families': 105,\n",
              "             'year': 106,\n",
              "             'how': 107,\n",
              "             'women': 108,\n",
              "             'nt': 109,\n",
              "             'see': 110,\n",
              "             'discuss': 111,\n",
              "             'America': 112,\n",
              "             'Happy': 113,\n",
              "             'honor': 114,\n",
              "             'need': 115,\n",
              "             'just': 116,\n",
              "             'hearing': 117,\n",
              "             'so': 118,\n",
              "             'In': 119,\n",
              "             'As': 120,\n",
              "             'years': 121,\n",
              "             'but': 122,\n",
              "             'country': 123,\n",
              "             'A': 124,\n",
              "             'would': 125,\n",
              "             '': 126,\n",
              "             'U.S.': 127,\n",
              "             'he': 128,\n",
              "             'than': 129,\n",
              "             'first': 130,\n",
              "             'what': 131,\n",
              "             'last': 132,\n",
              "             'working': 133,\n",
              "             'or': 134,\n",
              "             'meeting': 135,\n",
              "             'family': 136,\n",
              "             'Our': 137,\n",
              "             'm': 138,\n",
              "             't': 139,\n",
              "             \"n't\": 140,\n",
              "             'join': 141,\n",
              "             'health': 142,\n",
              "             'many': 143,\n",
              "             'bipartisan': 144,\n",
              "             'nation': 145,\n",
              "             'Congressional': 146,\n",
              "             'make': 147,\n",
              "             'proud': 148,\n",
              "             'like': 149,\n",
              "             'Day': 150,\n",
              "             \"'m\": 151,\n",
              "             'important': 152,\n",
              "             'National': 153,\n",
              "             '': 154,\n",
              "             'had': 155,\n",
              "             'On': 156,\n",
              "             'office': 157,\n",
              "             'over': 158,\n",
              "             'law': 159,\n",
              "             'w/': 160,\n",
              "             'get': 161,\n",
              "             '2018': 162,\n",
              "             '...': 163,\n",
              "             'across': 164,\n",
              "             'those': 165,\n",
              "             'passed': 166,\n",
              "             'which': 167,\n",
              "             \"'\": 168,\n",
              "             'members': 169,\n",
              "             'news': 170,\n",
              "             'legislation': 171,\n",
              "             'community': 172,\n",
              "             'Rep.': 173,\n",
              "             'vote': 174,\n",
              "             'Congratulations': 175,\n",
              "             'no': 176,\n",
              "             '@realDonaldTrump': 177,\n",
              "             'forward': 178,\n",
              "             'into': 179,\n",
              "             'protect': 180,\n",
              "             'good': 181,\n",
              "             '@POTUS': 182,\n",
              "             'its': 183,\n",
              "             'know': 184,\n",
              "             'Chairman': 185,\n",
              "             'million': 186,\n",
              "             'If': 187,\n",
              "             'against': 188,\n",
              "             '@HouseGOP': 189,\n",
              "             'federal': 190,\n",
              "             'were': 191,\n",
              "             'County': 192,\n",
              "             'service': 193,\n",
              "             '%': 194,\n",
              "             'why': 195,\n",
              "             'reform': 196,\n",
              "             'Committee': 197,\n",
              "             'right': 198,\n",
              "             'Proud': 199,\n",
              "             'colleagues': 200,\n",
              "             'government': 201,\n",
              "             'funding': 202,\n",
              "             'her': 203,\n",
              "             'these': 204,\n",
              "             'back': 205,\n",
              "             'care': 206,\n",
              "             '/': 207,\n",
              "             'state': 208,\n",
              "             'ICYMI': 209,\n",
              "             'joined': 210,\n",
              "             'every': 211,\n",
              "             'being': 212,\n",
              "             '1': 213,\n",
              "             'them': 214,\n",
              "             'meet': 215,\n",
              "             'economy': 216,\n",
              "             'continue': 217,\n",
              "             'Washington': 218,\n",
              "             'when': 219,\n",
              "             'take': 220,\n",
              "             'jobs': 221,\n",
              "             'keep': 222,\n",
              "             'Senate': 223,\n",
              "             'You': 224,\n",
              "             'local': 225,\n",
              "             '': 226,\n",
              "             'live': 227,\n",
              "             'School': 228,\n",
              "             'New': 229,\n",
              "             'public': 230,\n",
              "             'Republicans': 231,\n",
              "             'US': 232,\n",
              "             'school': 233,\n",
              "             'talk': 234,\n",
              "             'w': 235,\n",
              "             'military': 236,\n",
              "             'most': 237,\n",
              "             'Tax': 238,\n",
              "             'some': 239,\n",
              "             'visit': 240,\n",
              "             'lives': 241,\n",
              "             'only': 242,\n",
              "             'want': 243,\n",
              "             'Watch': 244,\n",
              "             'life': 245,\n",
              "             'businesses': 246,\n",
              "             'way': 247,\n",
              "             'He': 248,\n",
              "             '@SpeakerRyan': 249,\n",
              "             'small': 250,\n",
              "             '': 251,\n",
              "             'after': 252,\n",
              "             'during': 253,\n",
              "             'statement': 254,\n",
              "             'children': 255,\n",
              "             'hear': 256,\n",
              "             'if': 257,\n",
              "             'plan': 258,\n",
              "             'communities': 259,\n",
              "             'Tune': 260,\n",
              "             '2': 261,\n",
              "             'made': 262,\n",
              "             'men': 263,\n",
              "             'veterans': 264,\n",
              "             'there': 265,\n",
              "             'friend': 266,\n",
              "             'That': 267,\n",
              "             'celebrate': 268,\n",
              "             'because': 269,\n",
              "             'still': 270,\n",
              "             'GOP': 271,\n",
              "             'fight': 272,\n",
              "             'pay': 273,\n",
              "             'There': 274,\n",
              "             'gun': 275,\n",
              "             'hard': 276,\n",
              "             'down': 277,\n",
              "             'thank': 278,\n",
              "             'voted': 279,\n",
              "             'For': 280,\n",
              "             'staff': 281,\n",
              "             'tonight': 282,\n",
              "             'budget': 283,\n",
              "             \"'re\": 284,\n",
              "             'before': 285,\n",
              "             'off': 286,\n",
              "             'everyone': 287,\n",
              "             'night': 288,\n",
              "             'To': 289,\n",
              "             'end': 290,\n",
              "             'What': 291,\n",
              "             'leaders': 292,\n",
              "             'world': 293,\n",
              "             'DC': 294,\n",
              "             'full': 295,\n",
              "             'other': 296,\n",
              "             'go': 297,\n",
              "             'yesterday': 298,\n",
              "             'Democrats': 299,\n",
              "             'Last': 300,\n",
              "             'better': 301,\n",
              "             'does': 302,\n",
              "             'State': 303,\n",
              "             'Congressman': 304,\n",
              "             'Read': 305,\n",
              "             'home': 306,\n",
              "             'Here': 307,\n",
              "             'next': 308,\n",
              "             'look': 309,\n",
              "             're': 310,\n",
              "             'stand': 311,\n",
              "             'th': 312,\n",
              "             'since': 313,\n",
              "             'very': 314,\n",
              "             'workers': 315,\n",
              "             'GOPTaxScam': 316,\n",
              "             'opportunity': 317,\n",
              "             'high': 318,\n",
              "             'DACA': 319,\n",
              "             'High': 320,\n",
              "             'ensure': 321,\n",
              "             'through': 322,\n",
              "             'TaxReform': 323,\n",
              "             'Capitol': 324,\n",
              "             'job': 325,\n",
              "             'LIVE': 326,\n",
              "             'stop': 327,\n",
              "             'also': 328,\n",
              "             'part': 329,\n",
              "             'where': 330,\n",
              "             'including': 331,\n",
              "             'North': 332,\n",
              "             'ago': 333,\n",
              "             'together': 334,\n",
              "             'leadership': 335,\n",
              "             'Iran': 336,\n",
              "             'best': 337,\n",
              "             'action': 338,\n",
              "             'benefits': 339,\n",
              "             'thanks': 340,\n",
              "             'business': 341,\n",
              "             'o': 342,\n",
              "             'access': 343,\n",
              "             'future': 344,\n",
              "             'national': 345,\n",
              "             'two': 346,\n",
              "             '10': 347,\n",
              "             'More': 348,\n",
              "             'issues': 349,\n",
              "             'always': 350,\n",
              "             'letter': 351,\n",
              "             'sure': 352,\n",
              "             'TaxCutsandJobsAct': 353,\n",
              "             'another': 354,\n",
              "             'Administration': 355,\n",
              "             'efforts': 356,\n",
              "             'introduced': 357,\n",
              "             'long': 358,\n",
              "             'open': 359,\n",
              "             'program': 360,\n",
              "             'give': 361,\n",
              "             'Dr.': 362,\n",
              "             'decision': 363,\n",
              "             'violence': 364,\n",
              "             'provide': 365,\n",
              "             'And': 366,\n",
              "             'could': 367,\n",
              "             'again': 368,\n",
              "             'cuts': 369,\n",
              "             'Veterans': 370,\n",
              "             'With': 371,\n",
              "             'security': 372,\n",
              "             'ca': 373,\n",
              "             'said': 374,\n",
              "             '4': 375,\n",
              "             'put': 376,\n",
              "             'They': 377,\n",
              "             'call': 378,\n",
              "             'needs': 379,\n",
              "             'any': 380,\n",
              "             'learn': 381,\n",
              "             'too': 382,\n",
              "             'bills': 383,\n",
              "             'let': 384,\n",
              "             'de': 385,\n",
              "             'much': 386,\n",
              "             'young': 387,\n",
              "             'No': 388,\n",
              "             've': 389,\n",
              "             'making': 390,\n",
              "             'safe': 391,\n",
              "             'deserve': 392,\n",
              "             'serve': 393,\n",
              "             'discussion': 394,\n",
              "             'honored': 395,\n",
              "             '--': 396,\n",
              "             'met': 397,\n",
              "             'Congrats': 398,\n",
              "             'taxes': 399,\n",
              "             'strong': 400,\n",
              "             '': 401,\n",
              "             'H.R.': 402,\n",
              "             'around': 403,\n",
              "             'never': 404,\n",
              "             'May': 405,\n",
              "             'code': 406,\n",
              "             'lost': 407,\n",
              "             'speak': 408,\n",
              "             'town': 409,\n",
              "             'Do': 410,\n",
              "             'Join': 411,\n",
              "             'co': 412,\n",
              "             'use': 413,\n",
              "             'Yesterday': 414,\n",
              "             'money': 415,\n",
              "             \"'ve\": 416,\n",
              "             'infrastructure': 417,\n",
              "             'critical': 418,\n",
              "             'did': 419,\n",
              "             'At': 420,\n",
              "             'under': 421,\n",
              "             '': 422,\n",
              "             'm': 423,\n",
              "             'i': 424,\n",
              "             'April': 425,\n",
              "             'Good': 426,\n",
              "             'hosting': 427,\n",
              "             'spending': 428,\n",
              "             'United': 429,\n",
              "             'crisis': 430,\n",
              "             'taking': 431,\n",
              "             'prayers': 432,\n",
              "             'via': 433,\n",
              "             'District': 434,\n",
              "             'joining': 435,\n",
              "             'remember': 436,\n",
              "             'water': 437,\n",
              "             'tomorrow': 438,\n",
              "             'Secretary': 439,\n",
              "             'Texas': 440,\n",
              "             'Subcommittee': 441,\n",
              "             'States': 442,\n",
              "             'Bush': 443,\n",
              "             'class': 444,\n",
              "             'VA': 445,\n",
              "             'pass': 446,\n",
              "             'safety': 447,\n",
              "             'address': 448,\n",
              "             'hope': 449,\n",
              "             'Let': 450,\n",
              "             'him': 451,\n",
              "             'opioid': 452,\n",
              "             'Now': 453,\n",
              "             '2017': 454,\n",
              "             'King': 455,\n",
              "             'step': 456,\n",
              "             '3': 457,\n",
              "             'speaking': 458,\n",
              "             'economic': 459,\n",
              "             'having': 460,\n",
              "             'talking': 461,\n",
              "             'Art': 462,\n",
              "             \"'ll\": 463,\n",
              "             'going': 464,\n",
              "             'passing': 465,\n",
              "             '5': 466,\n",
              "             'First': 467,\n",
              "             'floor': 468,\n",
              "             'Health': 469,\n",
              "             'happy': 470,\n",
              "             'says': 471,\n",
              "             '@FoxNews': 472,\n",
              "             'friends': 473,\n",
              "             'well': 474,\n",
              "             'billion': 475,\n",
              "             'Republican': 476,\n",
              "             'afternoon': 477,\n",
              "             'employees': 478,\n",
              "             'Center': 479,\n",
              "             'Wishing': 480,\n",
              "             'importance': 481,\n",
              "             'questions': 482,\n",
              "             'system': 483,\n",
              "             's': 484,\n",
              "             'student': 485,\n",
              "             'thoughts': 486,\n",
              "             'Facebook': 487,\n",
              "             'deal': 488,\n",
              "             'enforcement': 489,\n",
              "             'month': 490,\n",
              "             'receive': 491,\n",
              "             'll': 492,\n",
              "             'free': 493,\n",
              "             'c': 494,\n",
              "             'coming': 495,\n",
              "             'big': 496,\n",
              "             'both': 497,\n",
              "             'come': 498,\n",
              "             'while': 499,\n",
              "             'fighting': 500,\n",
              "             'former': 501,\n",
              "             'millions': 502,\n",
              "             'These': 503,\n",
              "             'special': 504,\n",
              "             'Competition': 505,\n",
              "             'SOTU': 506,\n",
              "             'So': 507,\n",
              "             '@HouseCommerce': 508,\n",
              "             'Looking': 509,\n",
              "             'Honored': 510,\n",
              "             'SNAP': 511,\n",
              "             'immigration': 512,\n",
              "             'programs': 513,\n",
              "             'Jobs': 514,\n",
              "             'companies': 515,\n",
              "             'Please': 516,\n",
              "             'Security': 517,\n",
              "             'When': 518,\n",
              "             'find': 519,\n",
              "             'hall': 520,\n",
              "             'question': 521,\n",
              "             'report': 522,\n",
              "             'California': 523,\n",
              "             'h': 524,\n",
              "             '+': 525,\n",
              "             'event': 526,\n",
              "             'policy': 527,\n",
              "             'f': 528,\n",
              "             'history': 529,\n",
              "             'administration': 530,\n",
              "             'district': 531,\n",
              "             'doing': 532,\n",
              "             'celebrating': 533,\n",
              "             'From': 534,\n",
              "             'While': 535,\n",
              "             'victims': 536,\n",
              "             'constituents': 537,\n",
              "             'helping': 538,\n",
              "             'Check': 539,\n",
              "             'spoke': 540,\n",
              "             'NOW': 541,\n",
              "             'announced': 542,\n",
              "             'say': 543,\n",
              "             'cut': 544,\n",
              "             'agree': 545,\n",
              "             'bring': 546,\n",
              "             'supporting': 547,\n",
              "             'Just': 548,\n",
              "             'White': 549,\n",
              "             'latest': 550,\n",
              "             'real': 551,\n",
              "             'Pruitt': 552,\n",
              "             'education': 553,\n",
              "             'Bill': 554,\n",
              "             'team': 555,\n",
              "             'Russia': 556,\n",
              "             'continues': 557,\n",
              "             'Community': 558,\n",
              "             'Members': 559,\n",
              "             'place': 560,\n",
              "             'After': 561,\n",
              "             'signed': 562,\n",
              "             'T': 563,\n",
              "             'may': 564,\n",
              "             'own': 565,\n",
              "             're': 566,\n",
              "             'schools': 567,\n",
              "             'But': 568,\n",
              "             'Caucus': 569,\n",
              "             'M': 570,\n",
              "             'attack': 571,\n",
              "             'pleasure': 572,\n",
              "             'read': 573,\n",
              "             'annual': 574,\n",
              "             'able': 575,\n",
              "             'start': 576,\n",
              "             'hold': 577,\n",
              "             'birthday': 578,\n",
              "             'farmers': 579,\n",
              "             'Glad': 580,\n",
              "             'believe': 581,\n",
              "             'visiting': 582,\n",
              "             'Cuts': 583,\n",
              "             'even': 584,\n",
              "             'impact': 585,\n",
              "             'issue': 586,\n",
              "             'lead': 587,\n",
              "             'old': 588,\n",
              "             'change': 589,\n",
              "             'growth': 590,\n",
              "             'middle': 591,\n",
              "             'recognize': 592,\n",
              "             'Florida': 593,\n",
              "             'increase': 594,\n",
              "             'recent': 595,\n",
              "             'One': 596,\n",
              "             'clear': 597,\n",
              "             'pm': 598,\n",
              "             'She': 599,\n",
              "             'already': 600,\n",
              "             'residents': 601,\n",
              "             'Enjoyed': 602,\n",
              "             'teachers': 603,\n",
              "             'IRS': 604,\n",
              "             'rights': 605,\n",
              "             'b': 606,\n",
              "             'combat': 607,\n",
              "             'ever': 608,\n",
              "             'served': 609,\n",
              "             'brave': 610,\n",
              "             'few': 611,\n",
              "             'Barbara': 612,\n",
              "             'food': 613,\n",
              "             'top': 614,\n",
              "             'Award': 615,\n",
              "             'folks': 616,\n",
              "             'power': 617,\n",
              "             '6': 618,\n",
              "             'got': 619,\n",
              "             'seeing': 620,\n",
              "             'weekend': 621,\n",
              "             'anniversary': 622,\n",
              "             'story': 623,\n",
              "             'EPA': 624,\n",
              "             'San': 625,\n",
              "             'member': 626,\n",
              "             'taxreform': 627,\n",
              "             'trade': 628,\n",
              "             'getting': 629,\n",
              "             'sent': 630,\n",
              "             'tour': 631,\n",
              "             'BREAKING': 632,\n",
              "             'nearly': 633,\n",
              "             'without': 634,\n",
              "             'Israel': 635,\n",
              "             'Women': 636,\n",
              "             '@WaysandMeansGOP': 637,\n",
              "             'John': 638,\n",
              "             'calling': 639,\n",
              "             'show': 640,\n",
              "             'City': 641,\n",
              "             'called': 642,\n",
              "             'improve': 643,\n",
              "             'Hall': 644,\n",
              "             'Small': 645,\n",
              "             'incredible': 646,\n",
              "             'until': 647,\n",
              "             'Obama': 648,\n",
              "             'Special': 649,\n",
              "             'rule': 650,\n",
              "             'took': 651,\n",
              "             'g': 652,\n",
              "             'past': 653,\n",
              "             'allow': 654,\n",
              "             'done': 655,\n",
              "             'information': 656,\n",
              "             'led': 657,\n",
              "             'means': 658,\n",
              "             'Scott': 659,\n",
              "             'between': 660,\n",
              "             'commitment': 661,\n",
              "             'days': 662,\n",
              "             'investigation': 663,\n",
              "             'kids': 664,\n",
              "             'think': 665,\n",
              "             'y': 666,\n",
              "             'Saturday': 667,\n",
              "             'act': 668,\n",
              "             'committee': 669,\n",
              "             'energy': 670,\n",
              "             'she': 671,\n",
              "             'March': 672,\n",
              "             'giving': 673,\n",
              "             'Martin': 674,\n",
              "             'pro': 675,\n",
              "             '20': 676,\n",
              "             '8': 677,\n",
              "             'Learn': 678,\n",
              "             'amendment': 679,\n",
              "             'colleague': 680,\n",
              "             'create': 681,\n",
              "             'hardworking': 682,\n",
              "             'policies': 683,\n",
              "             'relief': 684,\n",
              "             'Had': 685,\n",
              "             'came': 686,\n",
              "             'Hill': 687,\n",
              "             'bonuses': 688,\n",
              "             'update': 689,\n",
              "             'Department': 690,\n",
              "             'Korea': 691,\n",
              "             'border': 692,\n",
              "             'love': 693,\n",
              "             'wo': 694,\n",
              "             'AM': 695,\n",
              "             'ed': 696,\n",
              "             'healthcare': 697,\n",
              "             'leading': 698,\n",
              "             'nuclear': 699,\n",
              "             'FBI': 700,\n",
              "             'TaxDay': 701,\n",
              "             'months': 702,\n",
              "             'same': 703,\n",
              "             'Member': 704,\n",
              "             'Office': 705,\n",
              "             'win': 706,\n",
              "             'held': 707,\n",
              "             'resources': 708,\n",
              "             'Mark': 709,\n",
              "             'away': 710,\n",
              "             'la': 711,\n",
              "             'pleased': 712,\n",
              "             'watch': 713,\n",
              "             'All': 714,\n",
              "             'conference': 715,\n",
              "             'discussing': 716,\n",
              "             'looking': 717,\n",
              "             'ready': 718,\n",
              "             'voting': 719,\n",
              "             'Mueller': 720,\n",
              "             'Syria': 721,\n",
              "             'due': 722,\n",
              "             'officials': 723,\n",
              "             'p': 724,\n",
              "             'shooting': 725,\n",
              "             'Director': 726,\n",
              "             'chance': 727,\n",
              "             '@FoxBusiness': 728,\n",
              "             '@HouseDemocrats': 729,\n",
              "             'each': 730,\n",
              "             'group': 731,\n",
              "             'political': 732,\n",
              "             'states': 733,\n",
              "             'such': 734,\n",
              "             'process': 735,\n",
              "             'raise': 736,\n",
              "             'welcome': 737,\n",
              "             'Jr.': 738,\n",
              "             'discussed': 739,\n",
              "             'grateful': 740,\n",
              "             'man': 741,\n",
              "             '9': 742,\n",
              "             'South': 743,\n",
              "             'move': 744,\n",
              "             'prevent': 745,\n",
              "             'protecting': 746,\n",
              "             'role': 747,\n",
              "             'president': 748,\n",
              "             'true': 749,\n",
              "             'conversation': 750,\n",
              "             'Friday': 751,\n",
              "             'How': 752,\n",
              "             'Luther': 753,\n",
              "             'NetNeutrality': 754,\n",
              "             'leader': 755,\n",
              "             'opening': 756,\n",
              "             'share': 757,\n",
              "             '': 758,\n",
              "             'Service': 759,\n",
              "             'makes': 760,\n",
              "             '1st': 761,\n",
              "             '@': 762,\n",
              "             'During': 763,\n",
              "             'Your': 764,\n",
              "             'glad': 765,\n",
              "             'justice': 766,\n",
              "             'thousands': 767,\n",
              "             'Medicare': 768,\n",
              "             'Mike': 769,\n",
              "             'historic': 770,\n",
              "             'https': 771,\n",
              "             'op': 772,\n",
              "             'research': 773,\n",
              "             '7': 774,\n",
              "             'Social': 775,\n",
              "             'announce': 776,\n",
              "             'Tomorrow': 777,\n",
              "             'election': 778,\n",
              "             'Every': 779,\n",
              "             'WATCH': 780,\n",
              "             'serving': 781,\n",
              "             'three': 782,\n",
              "             'visited': 783,\n",
              "             'Take': 784,\n",
              "             'fellow': 785,\n",
              "             'heard': 786,\n",
              "             'Mr.': 787,\n",
              "             'Valley': 788,\n",
              "             'fund': 789,\n",
              "             'human': 790,\n",
              "             'save': 791,\n",
              "             'D.C.': 792,\n",
              "             'along': 793,\n",
              "             'woman': 794,\n",
              "             '50': 795,\n",
              "             'https://t': 796,\n",
              "             'things': 797,\n",
              "             'Association': 798,\n",
              "             'Russian': 799,\n",
              "             'growing': 800,\n",
              "             'holding': 801,\n",
              "             'insurance': 802,\n",
              "             'rural': 803,\n",
              "             '@EPAScottPruitt': 804,\n",
              "             'industry': 805,\n",
              "             'Birthday': 806,\n",
              "             'Business': 807,\n",
              "             'child': 808,\n",
              "             'wonderful': 809,\n",
              "             'Week': 810,\n",
              "             'cost': 811,\n",
              "             'effort': 812,\n",
              "             'forget': 813,\n",
              "             'line': 814,\n",
              "             'asked': 815,\n",
              "             'yet': 816,\n",
              "             '': 817,\n",
              "             'K': 818,\n",
              "             'loved': 819,\n",
              "             'provides': 820,\n",
              "             'released': 821,\n",
              "             'worked': 822,\n",
              "             'Budget': 823,\n",
              "             'IN': 824,\n",
              "             'Mayor': 825,\n",
              "             'Town': 826,\n",
              "             'd': 827,\n",
              "             'Month': 828,\n",
              "             'paid': 829,\n",
              "             'standing': 830,\n",
              "             'taxpayer': 831,\n",
              "             'ways': 832,\n",
              "             '@HouseAppropsGOP': 833,\n",
              "             'General': 834,\n",
              "             'SmallBusinessWeek': 835,\n",
              "             '100': 836,\n",
              "             'Did': 837,\n",
              "             'anti': 838,\n",
              "             'e': 839,\n",
              "             'enjoyed': 840,\n",
              "             'press': 841,\n",
              "             'war': 842,\n",
              "             'An': 843,\n",
              "             'Year': 844,\n",
              "             'epidemic': 845,\n",
              "             'recently': 846,\n",
              "             'wages': 847,\n",
              "             '@SteveScalise': 848,\n",
              "             'attend': 849,\n",
              "             'data': 850,\n",
              "             'dedicated': 851,\n",
              "             'Military': 852,\n",
              "             'entire': 853,\n",
              "             'far': 854,\n",
              "             'legacy': 855,\n",
              "             'legislative': 856,\n",
              "             'once': 857,\n",
              "             'recovery': 858,\n",
              "             '2016': 859,\n",
              "             'Annual': 860,\n",
              "             'CEO': 861,\n",
              "             'Program': 862,\n",
              "             'face': 863,\n",
              "             'record': 864,\n",
              "             'PM': 865,\n",
              "             'broken': 866,\n",
              "             'term': 867,\n",
              "             'progress': 868,\n",
              "             'resolution': 869,\n",
              "             'winning': 870,\n",
              "             'Farm': 871,\n",
              "             'especially': 872,\n",
              "             'Big': 873,\n",
              "             'hours': 874,\n",
              "             'nothing': 875,\n",
              "             'rate': 876,\n",
              "             'ET': 877,\n",
              "             'Lady': 878,\n",
              "             'case': 879,\n",
              "             'committed': 880,\n",
              "             'interview': 881,\n",
              "             'response': 882,\n",
              "             '*': 883,\n",
              "             'Chair': 884,\n",
              "             'Federal': 885,\n",
              "             'God': 886,\n",
              "             'Joining': 887,\n",
              "             'Michigan': 888,\n",
              "             'income': 889,\n",
              "             'left': 890,\n",
              "             'major': 891,\n",
              "             'officers': 892,\n",
              "             'taxpayers': 893,\n",
              "             'veteran': 894,\n",
              "             'FarmBill': 895,\n",
              "             'Park': 896,\n",
              "             'Vietnam': 897,\n",
              "             'priorities': 898,\n",
              "             'proposal': 899,\n",
              "             'proposed': 900,\n",
              "             'then': 901,\n",
              "             'used': 902,\n",
              "             'R': 903,\n",
              "             'Ryan': 904,\n",
              "             'enough': 905,\n",
              "             'less': 906,\n",
              "             'marks': 907,\n",
              "             'message': 908,\n",
              "             'named': 909,\n",
              "             'solution': 910,\n",
              "             'ACA': 911,\n",
              "             'G': 912,\n",
              "             'West': 913,\n",
              "             'citizens': 914,\n",
              "             'Easter': 915,\n",
              "             'bad': 916,\n",
              "             'Services': 917,\n",
              "             'Tonight': 918,\n",
              "             'key': 919,\n",
              "             'troops': 920,\n",
              "             'By': 921,\n",
              "             'Graham': 922,\n",
              "             'force': 923,\n",
              "             'services': 924,\n",
              "             'voice': 925,\n",
              "             'wants': 926,\n",
              "             'TODAY': 927,\n",
              "             'affordable': 928,\n",
              "             'drug': 929,\n",
              "             'number': 930,\n",
              "             'online': 931,\n",
              "             'remain': 932,\n",
              "             '@WhiteHouse': 933,\n",
              "             'Floor': 934,\n",
              "             'Pleased': 935,\n",
              "             'common': 936,\n",
              "             'deeply': 937,\n",
              "             'dollars': 938,\n",
              "             'helped': 939,\n",
              "             'Congresswoman': 940,\n",
              "             'Why': 941,\n",
              "             'clean': 942,\n",
              "             'excited': 943,\n",
              "             'expand': 944,\n",
              "             'host': 945,\n",
              "             'ones': 946,\n",
              "             'times': 947,\n",
              "             'others': 948,\n",
              "             'passage': 949,\n",
              "             'soon': 950,\n",
              "             'Academy': 951,\n",
              "             'Police': 952,\n",
              "             'Wednesday': 953,\n",
              "             'close': 954,\n",
              "             'en': 955,\n",
              "             'environment': 956,\n",
              "             'heart': 957,\n",
              "             'introduce': 958,\n",
              "             'order': 959,\n",
              "             'vital': 960,\n",
              "             'wish': 961,\n",
              "             'Ohio': 962,\n",
              "             'Since': 963,\n",
              "             'appreciate': 964,\n",
              "             'current': 965,\n",
              "             'represent': 966,\n",
              "             'trillion': 967,\n",
              "             'using': 968,\n",
              "             '@OfficialCBC': 969,\n",
              "             'Always': 970,\n",
              "             'His': 971,\n",
              "             'Not': 972,\n",
              "             'Ranking': 973,\n",
              "             'fair': 974,\n",
              "             'helps': 975,\n",
              "             'low': 976,\n",
              "             'result': 977,\n",
              "             'Dreamers': 978,\n",
              "             'congressional': 979,\n",
              "             'r': 980,\n",
              "             'received': 981,\n",
              "             'send': 982,\n",
              "             'urging': 983,\n",
              "             'Monday': 984,\n",
              "             'concerns': 985,\n",
              "             'debate': 986,\n",
              "             'debt': 987,\n",
              "             'hosted': 988,\n",
              "             'list': 989,\n",
              "             'Another': 990,\n",
              "             'Central': 991,\n",
              "             'Co': 992,\n",
              "             'Flint': 993,\n",
              "             'Zuckerberg': 994,\n",
              "             'actions': 995,\n",
              "             'deadline': 996,\n",
              "             'level': 997,\n",
              "             'sexual': 998,\n",
              "             'sign': 999,\n",
              "             ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi-Ma3p0TaaS",
        "colab_type": "code",
        "outputId": "75e186b0-1614-425f-a3f7-b073f7a304c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_scores(lstm_model, test_iterator)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  14, 2565,    9,   41, 1432,   21,    4,   21,   67,   41, 1078, 1707,\n",
            "          41,  198,   21,    4,   21,   67,   21,   13,   21,  906,    5,   41,\n",
            "        3170,   11,  198,   21,    4,   21,   67,  733, 1717, 1037,  847,   16,\n",
            "        1271,    6,   10,    2,    2,    3], device='cuda:0')\n",
            "In 2017 # Nevadans in 10 counties , 7 in # NV02 , will have just 1 choice , it s unfair for them pay a penalty bc of Obama 's failed law ( 3/3 ) <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "I am sorry to hear about my friend & amp ; colleague @DorisMatsui s car accident .   I m glad to know she is doing well & amp ; wish her a speedy recovery <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Travel for def & amp ; mil personnel is not a https://t.co/ucKdra1SGZ , often it is mandatory & amp ; imperative to our security . I 'm glad we addressed # https://t.co/ucKdra1SGZ in # NDAA17 <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Like Orlando & amp ; Las Vegas , we mourn those we lost too soon & amp ; pledge support to the injured . But we can not accept these crimes as normal . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Call in now ! Join me for a Telephone # TownHall . Dial 1 - 877 - 229 - 8493 use https://t.co/ucKdra1SGZ # https://t.co/ucKdra1SGZ . Hear updates on my work in Cong  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "POTUS 's words demonstrate a moral compass where north is south , east is west . We must be of one course as a country - to condemn hate & amp ; racism . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            " With Congress & amp ; @POTUS working hand - in - hand , we have made great strides . I ca nt tell you how happy I am to see ou  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "If you clock - in & amp ; out , you deserve a # https://t.co/ucKdra1SGZ that allows you to survive & amp ; even get ahead . Coloradans voted to inc  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "In MN , # Medicaid covers : \n",
            "\n",
            "  1 in 4 children \n",
            "  1 in 2 nursing home residents \n",
            "  1 in 2 people with disabilities \n",
            "\n",
            " The  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Mr. https://t.co/ucKdra1SGZ was my 7th grade math teacher & amp ; he changed my life . He looked out for me & amp ; got me on right track . On  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Glad my Storm tweet 's been taken by the storm & amp ; is keeping your eye on the storm ( # Matthew ) . Stay vigilant ! https://t.co/ucKdra1SGZ . https://t.co/ucKdra1SGZ  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "When I was with others on the battlefield & amp ; we saw a chance to save a life , we did nt have a meeting about it ; we a  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "https://t.co/ucKdra1SGZ https://t.co/ucKdra1SGZ : Tomorrow at 6 pm , I 'm hosting a teletown hall to hear from you ! \n",
            " You can call directly to ( 888 ) 480 - 3626  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Take a few minutes to read my op - ed via @dallasnews . The battle over the # CFPB is not about right & amp ; left , it is abo  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "It s # SmallBusinessWeek . \n",
            "  https://t.co/ucKdra1SGZ % of small biz owners do nt think new tax law puts them on level field w/ big biz \n",
            "  https://t.co/ucKdra1SGZ  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "TODAY : \n",
            " 1 ) @YosemiteNPS has a FREE # EarthDay Festival , 10 am to 2 pm , at the Valley Visitor Center \n",
            " 2 ) https://t.co/ucKdra1SGZ is F  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "https://t.co/ucKdra1SGZ  The clock is ticking . https://t.co/ucKdra1SGZ  There are only 3 days left to sign up for health insurance . Do n't wait , act now by https://t.co/ucKdra1SGZ  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "My final vote as a member of the U.S. House of Representatives to bring clean water to the children of Flint , MI & amp ; keep govt open . # grateful <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "On # NationalWalkoutDay , I 'm carrying the photos & amp ; stories of Xavier Joy ( 23 ) , https://t.co/ucKdra1SGZ Holt ( 16 ) , https://t.co/ucKdra1SGZ Pendleton ( 15  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Do n't get me wrong - a low unemployment rate & amp ; a lower https://t.co/ucKdra1SGZ rate is great . But when do Americans get a raise ? A https://t.co/ucKdra1SGZ  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            " Habitat for Humanity is one of the best programs we have , and you all do a lot of good .  Great to see you all in t  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "\"  I knew , and I still feel , it 's the best job in the world ,  Tsongas said more than a decade after first taking off  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "As of Dec 15 , I 'll no longer tweet from this account . Please stay in touch by following me https://t.co/ucKdra1SGZ Thank you for the privilege . https://t.co/ucKdra1SGZ    <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "For https://t.co/ucKdra1SGZ ago , # Medicare has protected health & amp ; well - being of American families , saving lives , & amp ; improving nation 's economic security . 1/ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "To all the cops on the beat , and to your loved ones : You do not fight alone . We are with you , and behind you , https://t.co/ucKdra1SGZ  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "https://t.co/ucKdra1SGZ on   High - speed rail is about convenience ; water is a basic human need . Where do I get my  dams not trains  s  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "   Small Biz Confidence at All - Time High    \n",
            "\n",
            " Bloomberg on the @NFIB survey : \n",
            " \" ... producing one of the strongest read  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Great to meet https://t.co/ucKdra1SGZ & amp ; Angie https://t.co/ucKdra1SGZ & amp ; their kids https://t.co/ucKdra1SGZ , https://t.co/ucKdra1SGZ & amp ; https://t.co/ucKdra1SGZ today after their tour of @USCapitol . https://t.co/ucKdra1SGZ was  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Ruth https://t.co/ucKdra1SGZ But Ruth said ,  Do not urge me to leave you or turn back from following you ; for where you go , I will go ,  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "# https://t.co/ucKdra1SGZ is a big deal in # NC10 . I 'm proud to support the industry and all the good - paying jobs it has created & amp ;  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "# https://t.co/ucKdra1SGZ is https://t.co/ucKdra1SGZ to @JBSA_Official , NSA TX , https://t.co/ucKdra1SGZ military & amp ; civilian https://t.co/ucKdra1SGZ , the DoD s only Level 1 https://t.co/ucKdra1SGZ Center & amp ; 2  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "VOTE , VOTE , VOTE ! ! ! # TX22 Mayors , City Councils , School https://t.co/ucKdra1SGZ . Early voting began https://t.co/ucKdra1SGZ , ends https://t.co/ucKdra1SGZ .   5/5 final day . https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Is it too early for a # https://t.co/ucKdra1SGZ ?   Never .   Here is a picture of https://t.co/ucKdra1SGZ and me with a 4 District Constituent . I think h  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Erin , here s what I actually said : \n",
            "  I think their goal was chaos . To say that we have seen or read evidence that  w  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "If a Congressman is trying to https://t.co/ucKdra1SGZ you that America did nt  get had ,  give him the benefit of the doubt , maybe he  got had .  <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Meanwhile , state of good repair backlog of our nation 's # rail & amp ; # bus transit systems continues to grow $ 2.5 billion every single year . 2/ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "On this day in https://t.co/ucKdra1SGZ I came at the age of 8 from Puerto Rico to New York . On this day in https://t.co/ucKdra1SGZ I became a member of Congress . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Today is my https://t.co/ucKdra1SGZ s https://t.co/ucKdra1SGZ birthday ! https://t.co/ucKdra1SGZ , you re inspiration to me and my girls ! In her honor , I spoke on the House fl  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "https://t.co/ucKdra1SGZ great . Please call us at 202 - 225 - https://t.co/ucKdra1SGZ to schedule time to visit either in DC or https://t.co/ucKdra1SGZ . https://t.co/ucKdra1SGZ what is address of your church ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Check out this map of all the ' Farm to School ' gardens in OR ( there are more than 600 ! ) including quite a few in ou  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Praying for his wife Rose & amp ; all their loved ones . Blessed to have known James & amp ; forever grateful for his service . Rest in peace . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "The VA ( https://t.co/ucKdra1SGZ K employees & amp ; $ 180 billion annual budget ) has a sacred mission \" to care for him who has borne the battle  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "It 's # NationalParkWeek ! I 'm requesting   @USPS for a stamp to honor the https://t.co/ucKdra1SGZ citizens who were held at https://t.co/ucKdra1SGZ , now a Nat'l Park in HI . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "I join my fellow Americans in grieving the tragic loss of the men and women in Las Vegas . My thoughts go to their families & amp ; loved ones . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "My team & amp ; I work every day to be bridge for the Central Coast to the federal government & amp ; its resources . We break t  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Pay Raise ! Yes We Can ! That 's a great way to welcome a   New Year   ! Plus the @Eagles beat the https://t.co/ucKdra1SGZ ! 2017 https://t.co/ucKdra1SGZ  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "When sitting for hearings on # MuslimBan https://t.co/ucKdra1SGZ today , I hope # SCOTUS kept in mind the words & amp ; intent of the man who i m  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "@Sullied18 https://t.co/ucKdra1SGZ employment rate 2009 https://t.co/ucKdra1SGZ % \n",
            " https://t.co/ucKdra1SGZ employment rate dec 2017 https://t.co/ucKdra1SGZ % \n",
            " Today https://t.co/ucKdra1SGZ % .   So that 5 % drop https://t.co/ucKdra1SGZ to what exactly ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "\" Let 's face it , there is no Planet B \" - French President Emmanuel Macron to Congress today . \n",
            "\n",
            " He 's right , and the v  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Only way to stop executive gun control is to use the power of the purse .   Republicans took that off the table with the Omnibus . # tcot # 2A <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "It s back to Washington this week & amp ; I met up with members of the https://t.co/ucKdra1SGZ on my way to vote on the House Floor . Gr  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Sometimes people in Washington say something just ca nt be done . But I say , where there s a will , there s a way .   Y  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Best of luck to the defending 2017 # https://t.co/ucKdra1SGZ champions as they start the Stanley Cup https://t.co/ucKdra1SGZ ! # https://t.co/ucKdra1SGZ ! Get that three - https://t.co/ucKdra1SGZ ! I # https://t.co/ucKdra1SGZ . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "At this same point in their https://t.co/ucKdra1SGZ ... \n",
            " https://t.co/ucKdra1SGZ Obama had 79 % of his nominees approved \n",
            " https://t.co/ucKdra1SGZ George W. Bush had 65 % \n",
            " https://t.co/ucKdra1SGZ Cl  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "She brings him good , not harm , all the days of her life ... \n",
            " Honor her for all that her hands have done , and let her  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Matthew https://t.co/ucKdra1SGZ and 6 \n",
            " \n",
            " \" And the angel answered and said https://t.co/ucKdra1SGZ the women , https://t.co/ucKdra1SGZ not ye : for I know that ye seek Jesus , w  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "https://t.co/ucKdra1SGZ version : \" We 've decided $ 1,000 is n't going to do much for you . So give it to us because we can use it bette  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            ".. neo - Nazis , and the https://t.co/ucKdra1SGZ are a scourge on society and have no place in our country . These are not values we hold as Oregonians . 2/2 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "In 2017 , avg annual national premiums for Silver plan for a 40-year - old w/o a tax subsidy was $ https://t.co/ucKdra1SGZ . More than a $ 1,000 increase since 2014 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "I had the honor of flying with Captain https://t.co/ucKdra1SGZ , and it s with a heavy heart we say Godspeed to Andy , & amp ; the other b  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "Happy to report that my bipartisan bill with Rep. Ted Lieu ( D - CA ) , \" https://t.co/ucKdra1SGZ the State Department , \" has passed out of c  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "I m so proud of the 7 young men who are now Eagle Scouts .  Thrilled to speak at the Eagle Scout Court of Honor on S  https://t.co/ucKdra1SGZ K <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(0, device='cuda:0')\n",
            "If you 're in # NE02 and planning a trip to DC , my staff can help you schedule a Capitol and White House tour . Visit :  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Stopped by Monroe Co Fair and got to meet https://t.co/ucKdra1SGZ ( the goat ) who was being shown by https://t.co/ucKdra1SGZ ! Had a great day at the f  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "A 4-year - old girl and a 2-year - old boy shot and killed in the https://t.co/ucKdra1SGZ area yesterday . We can not go on ( and los  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "@Amtrak 's trans - Hudson track connects not just NY & amp ; NJ , but the entire region . Because the Gateway Project is so i m  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "https://t.co/ucKdra1SGZ : https://t.co/ucKdra1SGZ and I just voted for Cap - Were votes https://t.co/ucKdra1SGZ & 435 in our https://t.co/ucKdra1SGZ . The rush was just starting as people got out of work <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Like all the men and women who wear the badge in # California , I took an oath to uphold the law . That means all of t  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Our men & amp ; women in uniform https://t.co/ucKdra1SGZ enormous sacrifices to keep us safe . In return , we promise a fair shot at a go  https://t.co/ucKdra1SGZ m <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "It 's been my greatest privilege to serve the people of HI . Your https://t.co/ucKdra1SGZ & amp ; compassion have helped shape my incredible journey these 20 + years . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Spent my last day in Congress after 46 yrs w/ DC staff who 've become my dear family . So long gang ! See you up in NY  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "We https://t.co/ucKdra1SGZ major push for FL & amp ; PR to get fed disaster relief education funding . PR just got $ 600 M but FL still need  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "The problem is n't welfare , it 's wages ! Millions of Americans work 2 or more jobs & amp ; still depend on SNAP to help fee  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "Great working w/ VA & amp ; schools at the Diversity & amp ; https://t.co/ucKdra1SGZ Summit to ensure proper care for all veterans through VA - Med School https://t.co/ucKdra1SGZ . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "THREAD : When I saw this out - of - touch tweet , I thought about the people I 've met during my quest to # RaiseTheWage ...  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "I toured the https://t.co/ucKdra1SGZ VA this morning ; take a look at this video . They have a plan & amp ; things are moving forward . Al  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "# OTD in 1949 , # NATO was formed . The importance & amp ; impact of this partnership can not be overstated . In the interest o  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "CA , CT , IN , WA & amp ; OR have all adopted https://t.co/ucKdra1SGZ of the Gun Violence https://t.co/ucKdra1SGZ Order Act . They are saving lives wi  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "To # NY09 : As your Representative , I am working hard to fight against this budget & amp ; the effects of these tax cuts . I  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "We owe it to our states & amp ; local governments to ensure they are prepared with the knowledge & amp ; know - how to mitigate t  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "I hope to see you on May 14th at my town hall in Albany as I give an update on my work in Congress . https://t.co/ucKdra1SGZ the word  https://t.co/ucKdra1SGZ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "tensor(1, device='cuda:0')\n",
            "ACCURACY 0.676\n",
            "F1 SCORE 0.669\n",
            "PRECISION 0.665\n",
            "RECALL 0.672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.669"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWb0m_Eh_p5l",
        "colab_type": "text"
      },
      "source": [
        "# Attention Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKV-rtbf1ewa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "    \"\"\"\n",
        "    super(Encoder, self).__init__()\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n",
        "                      dropout=dropout, bidirectional=True)\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "    \"\"\"\n",
        "    outputs, finals = self.rnn(inputs)\n",
        "    return outputs, finals\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.attn = nn.Linear(hidden_size, hidden_size)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, dec_out, enc_outs):\n",
        "    \"\"\"\n",
        "      - `dec_out`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the decoder hidden state.\n",
        "      - `enc_outs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the encoder\n",
        "          outputs for each decoding step to attend to. \n",
        "    \"\"\"\n",
        "    weighted_enc_hidden = self.attn(enc_outs) #dims: batch x seq_len X hidden\n",
        "    energies = torch.bmm(weighted_enc_hidden, dec_out.permute(1, 2, 0)) #dims: batch x seq_len x 1\n",
        "    alphas = self.softmax(energies)\n",
        "    return alphas #dims: batch x seq_len x 1\n",
        "    \n",
        "class AttentionDecoder(nn.Module):\n",
        "  \"\"\"An attention-based RNN decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, attention=None, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "        - `attention`: this is your self-defined Attention object. You can\n",
        "            either define an individual class for your Attention and pass it\n",
        "            here or leave `attention` as None and just implement everything\n",
        "            here.\n",
        "    \"\"\"\n",
        "    super(AttentionDecoder, self).__init__()\n",
        "\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n",
        "                      dropout=dropout, bidirectional=False)\n",
        "    \n",
        "    self.attn = attention\n",
        "\n",
        "    if attention is None:\n",
        "      self.attn = Attention(input_size, hidden_size)\n",
        "\n",
        "    self.w = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    \n",
        "  def forward(self, inputs, encoder_hiddens, encoder_finals,  src_mask,\n",
        "              trg_mask, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "    \n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_hiddens`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the encoder\n",
        "          outputs for each decoding step to attend to. \n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `src_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n",
        "          representing the mask for source sentences.\n",
        "      - `trg_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n",
        "          representing the mask for target sentences.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: (same as in Decoder) a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      # max_len = trg_mask.size(-1)\n",
        "        max_len = inputs.size(1)\n",
        "\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "\n",
        "    outputs = []\n",
        "    all_alphas = []\n",
        "    \n",
        "    #todo: add connection of context to next hidden state\n",
        "    #todo maybe: use dec_out instead of hidden for attention \n",
        "\n",
        "    for i in range(max_len):\n",
        "      dec_out, hidden = self.rnn(inputs[:, i:i+1, :], hidden) # hidden dims: (1, batch_size, hidden_size)\n",
        "\n",
        "      alphas = self.attn(hidden, encoder_hiddens)  #dims: batch x seq_len x 1\n",
        "      all_alphas.append(alphas)\n",
        "      context = torch.bmm(encoder_hiddens.permute(0, 2, 1), alphas)  #dims: batch x hidden_size x 1\n",
        "      out = self.w(torch.cat((context.permute(2,0,1), hidden), dim=2)).squeeze(0) #dims: 1 x batch x hidden_size\n",
        "      outputs.append(out)\n",
        "\n",
        "    outputs = torch.stack(outputs, dim=1)  \n",
        "    return hidden, outputs, all_alphas\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = encoder_finals\n",
        "    ### Your code here!\n",
        "\n",
        "    return decoder_init_hiddens\n",
        "\n",
        "class EncoderAttentionDecoder(nn.Module):\n",
        "  \"\"\"A Encoder-Decoder architecture with attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed , trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: an `AttentionDecoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderAttentionDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and tar get sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    ### Your code here!\n",
        "    # You can refer to `EncoderDecoder` and extend from it.\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    return self.decode(encoder_hiddens, encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_hiddens, encoder_finals, trg_ids, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_hiddens, encoder_finals, None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZztZfagEhT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "LSTM_EMBED_DIM = 100\n",
        "LSTM_HIDDEN_DIM = 82\n",
        "LSTM_NUM_LAYERS = 1\n",
        "LSTM_DROPOUT = 0.3\n",
        "LSTM_LEARNING_RATE = 0.001\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "encoder = Encoder(LSTM_EMBED_DIM, LSTM_HIDDEN_DIM, LSTM_DROPOUT)\n",
        "decoder = AttentionDecoder(LSTM_EMBED_DIM, LSTM_HIDDEN_DIM, dropout = LSTM_DROPOUT)\n",
        "encdec  = EncoderAttentionDecoder(encoder, decoder, nn.Embedding.from_pretrained(embs_vocab), nn.Embedding.from_pretrained(embs_vocab) )\n",
        "\n",
        "# lstm_model = BiDiLSTM(embs_vocab, LSTM_EMBED_DIM, LSTM_HIDDEN_DIM, LSTM_NUM_LAYERS, NUM_CATEGORIES,bidirectional = BIDIRECTIONAL, dropout=LSTM_DROPOUT).to(device)\n",
        "# lstm_optimizer = optim.Adam(lstm_model.parameters(), lr = LSTM_LEARNING_RATE)\n",
        "# lstm_criterion = nn.CrossEntropyLoss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}